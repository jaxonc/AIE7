{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RAG System Comparison Analysis\n",
        "\n",
        "This notebook systematically compares Traditional RAG vs Knowledge Graph Enhanced RAG using GPT-4o for:\n",
        "1. **Prompt Generation**: Generate differentiating test prompts\n",
        "2. **Response Analysis**: Compare and evaluate system responses\n",
        "3. **Performance Assessment**: Analyze strengths/weaknesses of each approach\n",
        "\n",
        "## Comparison Flow:\n",
        "1. 🤖 **GPT-4o generates test prompts** for system comparison\n",
        "2. 👤 **User selects preferred prompt** from generated options\n",
        "3. 🔄 **Both RAG systems respond** to the selected prompt\n",
        "4. 📊 **GPT-4o analyzes responses** and provides detailed comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic imports and setup\n",
        "import asyncio\n",
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Import RAG components\n",
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "from aimakerspace.knowledge_graph import KnowledgeGraphEnhancedVectorDB\n",
        "from aimakerspace.openai_utils.prompts import UserRolePrompt, SystemRolePrompt\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "# Reload modules to ensure latest versions\n",
        "if 'aimakerspace.vectordatabase' in sys.modules:\n",
        "    importlib.reload(sys.modules['aimakerspace.vectordatabase'])\n",
        "    from aimakerspace.vectordatabase import VectorDatabase\n",
        "\n",
        "if 'aimakerspace.knowledge_graph' in sys.modules:\n",
        "    importlib.reload(sys.modules['aimakerspace.knowledge_graph'])\n",
        "    from aimakerspace.knowledge_graph import KnowledgeGraphEnhancedVectorDB\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI API Key setup\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "# Initialize ChatOpenAI for both RAG responses and GPT-4o analysis\n",
        "chat_openai = ChatOpenAI(model_name=\"gpt-4o-mini\")  # For RAG systems\n",
        "gpt4o_analyzer = ChatOpenAI(model_name=\"gpt-4o\")    # For prompt generation and analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process documents\n",
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "\n",
        "print(f\"📄 Loaded {len(documents)} document(s)\")\n",
        "print(f\"✂️ Split into {len(split_documents)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Traditional RAG Vector Database\n",
        "print(\"🔨 Building Traditional RAG Vector Database...\")\n",
        "traditional_vector_db = VectorDatabase()\n",
        "traditional_vector_db = asyncio.run(traditional_vector_db.abuild_from_list(split_documents))\n",
        "print(\"✅ Traditional RAG ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Knowledge Graph Enhanced RAG Vector Database\n",
        "print(\"🧠 Building Knowledge Graph Enhanced RAG Vector Database...\")\n",
        "kg_enhanced_db = KnowledgeGraphEnhancedVectorDB()\n",
        "kg_enhanced_db = asyncio.run(kg_enhanced_db.build_from_list(split_documents, num_categories=4))\n",
        "print(\"✅ Knowledge Graph RAG ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG System Templates and Pipelines\n",
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses detailed and comprehensive\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(RAG_SYSTEM_TEMPLATE)\n",
        "rag_user_prompt = UserRolePrompt(RAG_USER_TEMPLATE)\n",
        "\n",
        "class TraditionalRAGPipeline:\n",
        "    def __init__(self, vector_db: VectorDatabase, llm: ChatOpenAI):\n",
        "        self.vector_db = vector_db\n",
        "        self.llm = llm\n",
        "    \n",
        "    def run_pipeline(self, query: str, k: int = 3) -> dict:\n",
        "        # Get traditional vector search results\n",
        "        results = self.vector_db.search_by_text(query, k=k)\n",
        "        \n",
        "        # Build context\n",
        "        context_parts = []\n",
        "        for i, (text, score) in enumerate(results, 1):\n",
        "            context_parts.append(f\"[Source {i}]: {text}\")\n",
        "        \n",
        "        context_prompt = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Create messages for LLM\n",
        "        system_message = rag_system_prompt.create_message()\n",
        "        user_message = rag_user_prompt.create_message(\n",
        "            user_query=query,\n",
        "            context=context_prompt\n",
        "        )\n",
        "        \n",
        "        # Get LLM response\n",
        "        response = self.llm.run([system_message, user_message])\n",
        "        \n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"context\": results,\n",
        "            \"method\": \"Traditional RAG\"\n",
        "        }\n",
        "\n",
        "class KnowledgeGraphRAGPipeline:\n",
        "    def __init__(self, kg_db: KnowledgeGraphEnhancedVectorDB, llm: ChatOpenAI):\n",
        "        self.kg_db = kg_db\n",
        "        self.llm = llm\n",
        "    \n",
        "    def run_pipeline(self, query: str, k: int = 3) -> dict:\n",
        "        # Get graph-enhanced results with entity expansion\n",
        "        results = self.kg_db.search_with_graph_expansion(query, k=k)\n",
        "        \n",
        "        # Build context with entity information\n",
        "        context_parts = []\n",
        "        for i, (text, score, metadata) in enumerate(results, 1):\n",
        "            entities = metadata.get('entities', [])\n",
        "            category = metadata.get('category_name', 'Unknown')\n",
        "            \n",
        "            entity_info = f\" [Key entities: {', '.join(entities[:3])}]\" if entities else \"\"\n",
        "            context_parts.append(f\"[Source {i} - {category}]{entity_info}: {text}\")\n",
        "        \n",
        "        context_prompt = \"\\n\\n\".join(context_parts)\n",
        "         \n",
        "        # Create messages for LLM\n",
        "        system_message = rag_system_prompt.create_message()\n",
        "        user_message = rag_user_prompt.create_message(\n",
        "            user_query=query,\n",
        "            context=context_prompt\n",
        "        )\n",
        "        \n",
        "        # Get LLM response\n",
        "        response = self.llm.run([system_message, user_message])\n",
        "        \n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"context\": results,\n",
        "            \"entities_used\": [metadata.get('entities', []) for _, _, metadata in results],\n",
        "            \"categories_used\": [metadata.get('category_name', 'Unknown') for _, _, metadata in results],\n",
        "            \"method\": \"Knowledge Graph RAG\"\n",
        "        }\n",
        "\n",
        "# Initialize both pipelines\n",
        "traditional_rag = TraditionalRAGPipeline(traditional_vector_db, chat_openai)\n",
        "kg_rag = KnowledgeGraphRAGPipeline(kg_enhanced_db, chat_openai)\n",
        "\n",
        "print(\"🚀 Both RAG pipelines initialized and ready for comparison!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Generate Test Prompts with GPT-4o\n",
        "\n",
        "Using GPT-4o to generate sophisticated test prompts that will highlight the differences between traditional RAG and knowledge graph enhanced RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User input for document URL (or description)\n",
        "document_description = input(\"Please provide the document URL or description for context (e.g., 'PMarca Blog Archives - startup advice from Marc Andreessen'): \")\n",
        "\n",
        "if not document_description:\n",
        "    document_description = \"PMarca Blog Archives - startup advice and business insights from Marc Andreessen\"\n",
        "\n",
        "print(f\"📄 Using document description: {document_description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test prompts using GPT-4o\n",
        "prompt_generation_template = f\"\"\"Given the provided document, what would be a few example prompts to show the differing capabilities of a knowledge graph augmented rag system vs a traditional rag system?\n",
        "\n",
        "Document: {document_description}\n",
        "\n",
        "Please provide 5 different prompts that would highlight the advantages of knowledge graph enhanced RAG, such as:\n",
        "- Entity relationship understanding\n",
        "- Cross-concept connections\n",
        "- Category-based insights\n",
        "- Complex multi-step reasoning\n",
        "- Semantic understanding beyond keyword matching\n",
        "\n",
        "Format your response as:\n",
        "1. [Prompt 1]\n",
        "2. [Prompt 2]\n",
        "3. [Prompt 3]\n",
        "4. [Prompt 4]\n",
        "5. [Prompt 5]\n",
        "\n",
        "Each prompt should be designed to reveal meaningful differences between the two approaches.\n",
        "Keep in mind that the following system prompt is used for both RAG systems:\n",
        "{RAG_SYSTEM_TEMPLATE}\n",
        "\n",
        "We would like to avoid unanswerable prompts.\n",
        "\"\"\"\n",
        "\n",
        "print(\"🤖 Generating test prompts with GPT-4o...\")\n",
        "generated_prompts_response = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": prompt_generation_template}])\n",
        "print(\"\\n📝 Generated Test Prompts:\")\n",
        "print(\"=\" * 60)\n",
        "print(generated_prompts_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: User Prompt Selection\n",
        "\n",
        "Select which generated prompt you'd like to use for the comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract prompts from the response for easier selection\n",
        "import re\n",
        "\n",
        "# Parse the numbered prompts\n",
        "prompt_pattern = r'\\d+\\.\\s*(.+?)(?=\\n\\d+\\.|$)'\n",
        "matches = re.findall(prompt_pattern, generated_prompts_response, re.DOTALL)\n",
        "\n",
        "if matches:\n",
        "    prompts_list = [match.strip() for match in matches]\n",
        "    \n",
        "    print(\"\\n🎯 Available Prompts for Testing:\")\n",
        "    print(\"=\" * 50)\n",
        "    for i, prompt in enumerate(prompts_list, 1):\n",
        "        print(f\"{i}. {prompt}\")\n",
        "        print(\"-\" * 30)\n",
        "    \n",
        "    # Get user selection\n",
        "    while True:\n",
        "        try:\n",
        "            choice = int(input(f\"\\nSelect a prompt number (1-{len(prompts_list)}): \"))\n",
        "            if 1 <= choice <= len(prompts_list):\n",
        "                selected_prompt = prompts_list[choice - 1]\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Please enter a number between 1 and {len(prompts_list)}\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number\")\n",
        "else:\n",
        "    # Fallback if parsing fails\n",
        "    selected_prompt = input(\"\\nPlease copy and paste your preferred prompt from above: \")\n",
        "\n",
        "print(f\"\\n✅ Selected Prompt: {selected_prompt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Run Both RAG Systems\n",
        "\n",
        "Execute the selected prompt on both Traditional RAG and Knowledge Graph Enhanced RAG systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔄 Running both RAG systems with selected prompt...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Run Traditional RAG\n",
        "print(\"\\n🔍 Traditional RAG Processing...\")\n",
        "traditional_result = traditional_rag.run_pipeline(selected_prompt, k=3)\n",
        "print()\n",
        "\n",
        "# Run Knowledge Graph RAG\n",
        "print(\"🧠 Knowledge Graph RAG Processing...\")\n",
        "kg_result = kg_rag.run_pipeline(selected_prompt, k=3)\n",
        "\n",
        "print(\"✅ Both systems completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display both responses for comparison\n",
        "print(\"\\n📊 RAG SYSTEM RESPONSES COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n🔹 TRADITIONAL RAG RESPONSE:\")\n",
        "print(\"-\" * 50)\n",
        "print(traditional_result['response'])\n",
        "\n",
        "print(\"\\n\\n🔹 KNOWLEDGE GRAPH RAG RESPONSE:\")\n",
        "print(\"-\" * 50)\n",
        "print(kg_result['response'])\n",
        "\n",
        "# Show additional context info for KG RAG\n",
        "if 'categories_used' in kg_result:\n",
        "    unique_categories = set(kg_result['categories_used'])\n",
        "    unique_entities = set([entity for entities in kg_result['entities_used'] for entity in entities])\n",
        "    \n",
        "    print(f\"\\n🏷️ KG RAG Additional Context:\")\n",
        "    print(f\"   Categories Used: {', '.join(unique_categories)}\")\n",
        "    print(f\"   Unique Entities: {len(unique_entities)} entities\")\n",
        "    print(f\"   Top Entities: {', '.join(list(unique_entities)[:8])}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: GPT-4o Analysis and Comparison\n",
        "\n",
        "Using GPT-4o to provide detailed analysis and comparison of both responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive analysis prompt for GPT-4o\n",
        "analysis_template = f\"\"\"Given the following document used to provide context to our RAG systems, and prompt used to obtain a response, please analyze, compare, and rate the following responses while pointing out any significant differences between them and how the different approaches may have influenced this including potential pros and cons of both.\n",
        "\n",
        "Document: {document_description}\n",
        "\n",
        "Prompt: {selected_prompt}\n",
        "\n",
        "<Start traditional RAG response>\n",
        "{traditional_result['response']}\n",
        "<End traditional RAG response>\n",
        "\n",
        "<Start Knowledge Graph Augmented RAG response>\n",
        "{kg_result['response']}\n",
        "<End Knowledge Graph Augmented RAG response>\n",
        "\n",
        "Please provide a detailed analysis covering:\n",
        "\n",
        "1. **Response Quality Comparison**: Which response better addresses the prompt and why?\n",
        "2. **Content Depth**: Compare the depth and comprehensiveness of each response\n",
        "3. **Accuracy Assessment**: Evaluate the accuracy and relevance of information provided\n",
        "4. **Approach Differences**: How did each RAG approach influence the response?\n",
        "5. **Strengths & Weaknesses**: Key advantages and limitations of each approach\n",
        "6. **Use Case Recommendations**: When would each approach be more suitable?\n",
        "7. **Overall Rating**: Rate each response (1-10) with justification\n",
        "\n",
        "Please be specific about how the knowledge graph enhancement (entity extraction, relationship mapping, graph-based community detection clustering) affected the response quality compared to traditional vector similarity search.\"\"\"\n",
        "\n",
        "print(\"🧠 Generating detailed analysis with GPT-4o...\")\n",
        "analysis_response = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": analysis_template}])\n",
        "\n",
        "print(\"\\n📊 GPT-4o COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(analysis_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Additional Analysis: Context Comparison\n",
        "\n",
        "Let's also examine the specific contexts each system retrieved to understand the differences in retrieval strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n🔍 CONTEXT RETRIEVAL COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n🔹 TRADITIONAL RAG CONTEXT SOURCES:\")\n",
        "print(\"-\" * 40)\n",
        "for i, (text, score) in enumerate(traditional_result['context'], 1):\n",
        "    print(f\"Source {i} (Similarity Score: {score:.3f}):\")\n",
        "    print(f\"   📄 Content: {text[:200]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n🔹 KNOWLEDGE GRAPH RAG CONTEXT SOURCES:\")\n",
        "print(\"-\" * 40)\n",
        "for i, (text, score, metadata) in enumerate(kg_result['context'], 1):\n",
        "    category = metadata.get('category_name', 'Unknown')\n",
        "    entities = metadata.get('entities', [])\n",
        "    print(f\"Source {i} (Similarity Score: {score:.3f}):\")\n",
        "    print(f\"   📂 Semantic Category: {category}\")\n",
        "    if entities:\n",
        "        print(f\"   🏷️  Key Entities: {', '.join(entities[:5])}\")\n",
        "    print(f\"   📄 Content: {text[:200]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate context analysis\n",
        "context_analysis_template = f\"\"\"Please analyze the context retrieval strategies of these two RAG systems based on the sources they selected:\n",
        "\n",
        "Query: {selected_prompt}\n",
        "\n",
        "Traditional RAG Sources (similarity-based):\n",
        "{chr(10).join([f\"Source {i+1} (Score: {score:.3f}): {text[:150]}...\" for i, (text, score) in enumerate(traditional_result['context'])])}\n",
        "\n",
        "Knowledge Graph RAG Sources (entity + similarity-based):\n",
        "{chr(10).join([f\"Source {i+1} (Score: {score:.3f}, Category: {metadata.get('category_name', 'Unknown')}, Entities: {', '.join(metadata.get('entities', [])[:3])}): {text[:150]}...\" for i, (text, score, metadata) in enumerate(kg_result['context'])])}\n",
        "\n",
        "Please compare:\n",
        "1. How different are the retrieved contexts?\n",
        "2. Which approach found more relevant information for this specific query?\n",
        "3. How did entity extraction and graph-based community detection influence the Knowledge Graph RAG's choices?\n",
        "4. Are there any important perspectives or information that one system missed?\n",
        "\n",
        "Provide specific insights about the retrieval strategy differences.\"\"\"\n",
        "\n",
        "context_analysis = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": context_analysis_template}])\n",
        "\n",
        "print(\"\\n📊 CONTEXT RETRIEVAL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(context_analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "This comparison analysis demonstrates the practical differences between traditional vector similarity RAG and knowledge graph enhanced RAG approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n🎯 COMPARISON SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Selected Prompt: {selected_prompt}\")\n",
        "print(f\"Document Context: {document_description}\")\n",
        "print(\"\\n✅ Analysis Complete!\")\n",
        "print(\"🔄 To run another comparison, restart from Step 1 with a different prompt\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
