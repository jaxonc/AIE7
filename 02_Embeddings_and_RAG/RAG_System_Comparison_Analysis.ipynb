{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RAG System Comparison Analysis\n",
        "\n",
        "This notebook systematically compares Traditional RAG vs Knowledge Graph Enhanced RAG using GPT-4o for:\n",
        "1. **Prompt Generation**: Generate differentiating test prompts\n",
        "2. **Response Analysis**: Compare and evaluate system responses\n",
        "3. **Performance Assessment**: Analyze strengths/weaknesses of each approach\n",
        "\n",
        "## üöÄ Enhanced Knowledge Graph RAG Features:\n",
        "- **K-means clustering** on graph embeddings (degree, PageRank, clustering coefficient)\n",
        "- **Graph-based entity relationships** and query expansion\n",
        "- **Semantic category naming** for improved context understanding\n",
        "\n",
        "## Comparison Flow:\n",
        "1. ü§ñ **GPT-4o generates test prompts** for system comparison\n",
        "2. üë§ **User selects preferred prompt** from generated options\n",
        "3. üîÑ **Both RAG systems respond** to the selected prompt\n",
        "4. üìä **GPT-4o analyzes responses** and provides detailed comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic imports and setup\n",
        "import asyncio\n",
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Import RAG components\n",
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "from aimakerspace.knowledge_graph import KnowledgeGraphEnhancedVectorDB\n",
        "from aimakerspace.openai_utils.prompts import UserRolePrompt, SystemRolePrompt\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "# Reload modules to ensure latest versions\n",
        "if 'aimakerspace.vectordatabase' in sys.modules:\n",
        "    importlib.reload(sys.modules['aimakerspace.vectordatabase'])\n",
        "    from aimakerspace.vectordatabase import VectorDatabase\n",
        "\n",
        "if 'aimakerspace.knowledge_graph' in sys.modules:\n",
        "    importlib.reload(sys.modules['aimakerspace.knowledge_graph'])\n",
        "    from aimakerspace.knowledge_graph import KnowledgeGraphEnhancedVectorDB\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI API Key setup\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "# Initialize ChatOpenAI for both RAG responses and GPT-4o analysis\n",
        "chat_openai = ChatOpenAI(model_name=\"gpt-4o-mini\")  # For RAG systems\n",
        "gpt4o_analyzer = ChatOpenAI(model_name=\"gpt-4o\")    # For prompt generation and analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process documents\n",
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "\n",
        "print(f\"üìÑ Loaded {len(documents)} document(s)\")\n",
        "print(f\"‚úÇÔ∏è Split into {len(split_documents)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Traditional RAG Vector Database\n",
        "print(\"üî® Building Traditional RAG Vector Database...\")\n",
        "traditional_vector_db = VectorDatabase()\n",
        "traditional_vector_db = asyncio.run(traditional_vector_db.abuild_from_list(split_documents))\n",
        "print(\"‚úÖ Traditional RAG ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Knowledge Graph Enhanced RAG Vector Database with K-means Clustering\n",
        "print(\"üß† Building Knowledge Graph Enhanced RAG Vector Database...\")\n",
        "print(\"üéØ Using K-means clustering on graph embeddings for intelligent categorization...\")\n",
        "kg_enhanced_db = KnowledgeGraphEnhancedVectorDB()\n",
        "kg_enhanced_db = asyncio.run(kg_enhanced_db.build_from_list(split_documents, num_categories=4))\n",
        "print(\"‚úÖ Knowledge Graph RAG with K-means clustering ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG System Templates and Pipelines\n",
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses detailed and comprehensive\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(RAG_SYSTEM_TEMPLATE)\n",
        "rag_user_prompt = UserRolePrompt(RAG_USER_TEMPLATE)\n",
        "\n",
        "class TraditionalRAGPipeline:\n",
        "    def __init__(self, vector_db: VectorDatabase, llm: ChatOpenAI):\n",
        "        self.vector_db = vector_db\n",
        "        self.llm = llm\n",
        "    \n",
        "    def run_pipeline(self, query: str, k: int = 3) -> dict:\n",
        "        # Get traditional vector search results\n",
        "        results = self.vector_db.search_by_text(query, k=k)\n",
        "        \n",
        "        # Build context\n",
        "        context_parts = []\n",
        "        for i, (text, score) in enumerate(results, 1):\n",
        "            context_parts.append(f\"[Source {i}]: {text}\")\n",
        "        \n",
        "        context_prompt = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Create messages for LLM\n",
        "        system_message = rag_system_prompt.create_message()\n",
        "        user_message = rag_user_prompt.create_message(\n",
        "            user_query=query,\n",
        "            context=context_prompt\n",
        "        )\n",
        "        \n",
        "        # Get LLM response\n",
        "        response = self.llm.run([system_message, user_message])\n",
        "        \n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"context\": results,\n",
        "            \"method\": \"Traditional RAG\"\n",
        "        }\n",
        "\n",
        "class KnowledgeGraphRAGPipeline:\n",
        "    def __init__(self, kg_db: KnowledgeGraphEnhancedVectorDB, llm: ChatOpenAI):\n",
        "        self.kg_db = kg_db\n",
        "        self.llm = llm\n",
        "    \n",
        "    def run_pipeline(self, query: str, k: int = 3) -> dict:\n",
        "        # Get graph-enhanced results with entity expansion\n",
        "        results = self.kg_db.search_with_graph_expansion(query, k=k)\n",
        "        \n",
        "        # Build context with entity information\n",
        "        context_parts = []\n",
        "        for i, (text, score, metadata) in enumerate(results, 1):\n",
        "            entities = metadata.get('entities', [])\n",
        "            category = metadata.get('category_name', 'Unknown')\n",
        "            \n",
        "            entity_info = f\" [Key entities: {', '.join(entities[:3])}]\" if entities else \"\"\n",
        "            context_parts.append(f\"[Source {i} - {category}]{entity_info}: {text}\")\n",
        "        \n",
        "        context_prompt = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Create messages for LLM\n",
        "        system_message = rag_system_prompt.create_message()\n",
        "        user_message = rag_user_prompt.create_message(\n",
        "            user_query=query,\n",
        "            context=context_prompt\n",
        "        )\n",
        "        \n",
        "        # Get LLM response\n",
        "        response = self.llm.run([system_message, user_message])\n",
        "        \n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"context\": results,\n",
        "            \"entities_used\": [metadata.get('entities', []) for _, _, metadata in results],\n",
        "            \"categories_used\": [metadata.get('category_name', 'Unknown') for _, _, metadata in results],\n",
        "            \"method\": \"Knowledge Graph RAG\"\n",
        "        }\n",
        "\n",
        "# Initialize both pipelines\n",
        "traditional_rag = TraditionalRAGPipeline(traditional_vector_db, chat_openai)\n",
        "kg_rag = KnowledgeGraphRAGPipeline(kg_enhanced_db, chat_openai)\n",
        "\n",
        "print(\"üöÄ Both RAG pipelines initialized and ready for comparison!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Generate Test Prompts with GPT-4o\n",
        "\n",
        "Using GPT-4o to generate sophisticated test prompts that will highlight the differences between traditional RAG and knowledge graph enhanced RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User input for document URL (or description)\n",
        "document_description = input(\"Please provide the document URL or description for context (e.g., 'PMarca Blog Archives - startup advice from Marc Andreessen'): \")\n",
        "\n",
        "if not document_description:\n",
        "    document_description = \"PMarca Blog Archives - startup advice and business insights from Marc Andreessen\"\n",
        "\n",
        "print(f\"üìÑ Using document description: {document_description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test prompts using GPT-4o\n",
        "prompt_generation_template = f\"\"\"Given the provided document, what would be a few example prompts to show the differing capabilities of a knowledge graph augmented rag system vs a traditional rag system?\n",
        "\n",
        "Document: {document_description}\n",
        "\n",
        "Please provide 5 different prompts that would highlight the advantages of knowledge graph enhanced RAG, such as:\n",
        "- Entity relationship understanding\n",
        "- Cross-concept connections\n",
        "- Category-based insights\n",
        "- Complex multi-step reasoning\n",
        "- Semantic understanding beyond keyword matching\n",
        "\n",
        "Format your response as:\n",
        "1. [Prompt 1]\n",
        "2. [Prompt 2]\n",
        "3. [Prompt 3]\n",
        "4. [Prompt 4]\n",
        "5. [Prompt 5]\n",
        "\n",
        "We would like to avoid unanswerable prompts so keep in mind that the following system prompt is used for both RAG systems:\n",
        "<start RAG system prompt>\n",
        "{RAG_SYSTEM_TEMPLATE}\n",
        "</end RAG system prompt>\n",
        "Each prompt should be designed to reveal meaningful differences between the two approaches.\"\"\"\n",
        "\n",
        "print(\"ü§ñ Generating test prompts with GPT-4o...\")\n",
        "generated_prompts_response = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": prompt_generation_template}])\n",
        "print(\"\\nüìù Generated Test Prompts:\")\n",
        "print(\"=\" * 60)\n",
        "print(generated_prompts_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: User Prompt Selection\n",
        "\n",
        "Select which generated prompt you'd like to use for the comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract prompts from the response for easier selection\n",
        "import re\n",
        "\n",
        "# Parse the numbered prompts\n",
        "prompt_pattern = r'\\d+\\.\\s*(.+?)(?=\\n\\d+\\.|$)'\n",
        "matches = re.findall(prompt_pattern, generated_prompts_response, re.DOTALL)\n",
        "\n",
        "if matches:\n",
        "    prompts_list = [match.strip() for match in matches]\n",
        "    \n",
        "    print(\"\\nüéØ Available Prompts for Testing:\")\n",
        "    print(\"=\" * 50)\n",
        "    for i, prompt in enumerate(prompts_list, 1):\n",
        "        print(f\"{i}. {prompt}\")\n",
        "        print(\"-\" * 30)\n",
        "    \n",
        "    # Get user selection\n",
        "    while True:\n",
        "        try:\n",
        "            choice = int(input(f\"\\nSelect a prompt number (1-{len(prompts_list)}): \"))\n",
        "            if 1 <= choice <= len(prompts_list):\n",
        "                selected_prompt = prompts_list[choice - 1]\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Please enter a number between 1 and {len(prompts_list)}\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number\")\n",
        "else:\n",
        "    # Fallback if parsing fails\n",
        "    selected_prompt = input(\"\\nPlease copy and paste your preferred prompt from above: \")\n",
        "\n",
        "print(f\"\\n‚úÖ Selected Prompt: {selected_prompt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Run Both RAG Systems\n",
        "\n",
        "Execute the selected prompt on both Traditional RAG and Knowledge Graph Enhanced RAG systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Running both RAG systems with selected prompt...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Run Traditional RAG\n",
        "print(\"\\nüîç Traditional RAG Processing...\")\n",
        "traditional_result = traditional_rag.run_pipeline(selected_prompt, k=3)\n",
        "\n",
        "# Run Knowledge Graph RAG\n",
        "print(\"üß† Knowledge Graph RAG Processing...\")\n",
        "kg_result = kg_rag.run_pipeline(selected_prompt, k=3)\n",
        "\n",
        "print(\"‚úÖ Both systems completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display both responses for comparison\n",
        "print(\"\\nüìä RAG SYSTEM RESPONSES COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüîπ TRADITIONAL RAG RESPONSE:\")\n",
        "print(\"-\" * 50)\n",
        "print(traditional_result['response'])\n",
        "\n",
        "print(\"\\n\\nüîπ KNOWLEDGE GRAPH RAG RESPONSE:\")\n",
        "print(\"-\" * 50)\n",
        "print(kg_result['response'])\n",
        "\n",
        "# Show additional context info for KG RAG\n",
        "if 'categories_used' in kg_result:\n",
        "    unique_categories = set(kg_result['categories_used'])\n",
        "    unique_entities = set([entity for entities in kg_result['entities_used'] for entity in entities])\n",
        "    \n",
        "    print(f\"\\nüè∑Ô∏è KG RAG Additional Context:\")\n",
        "    print(f\"   Categories Used: {', '.join(unique_categories)}\")\n",
        "    print(f\"   Unique Entities: {len(unique_entities)} entities\")\n",
        "    print(f\"   Top Entities: {', '.join(list(unique_entities)[:8])}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: GPT-4o Analysis and Comparison\n",
        "\n",
        "Using GPT-4o to provide detailed analysis and comparison of both responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive analysis prompt for GPT-4o\n",
        "analysis_template = f\"\"\"Given the following document used to provide context to our RAG systems, and prompt used to obtain a response, please analyze, compare, and rate the following responses while pointing out any significant differences between them and how the different approaches may have influenced this including potential pros and cons of both.\n",
        "\n",
        "Document: {document_description}\n",
        "\n",
        "Prompt: {selected_prompt}\n",
        "\n",
        "<Start traditional RAG response>\n",
        "{traditional_result['response']}\n",
        "<End traditional RAG response>\n",
        "\n",
        "<Start Knowledge Graph Augmented RAG response>\n",
        "{kg_result['response']}\n",
        "<End Knowledge Graph Augmented RAG response>\n",
        "\n",
        "Please provide a detailed analysis covering:\n",
        "\n",
        "1. **Response Quality Comparison**: Which response better addresses the prompt and why?\n",
        "2. **Content Depth**: Compare the depth and comprehensiveness of each response\n",
        "3. **Accuracy Assessment**: Evaluate the accuracy and relevance of information provided\n",
        "4. **Approach Differences**: How did each RAG approach influence the response?\n",
        "5. **Strengths & Weaknesses**: Key advantages and limitations of each approach\n",
        "6. **Use Case Recommendations**: When would each approach be more suitable?\n",
        "7. **Overall Rating**: Rate each response (1-10) with justification\n",
        "\"\"\"\n",
        "\n",
        "print(\"üß† Generating detailed analysis with GPT-4o...\")\n",
        "analysis_response = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": analysis_template}])\n",
        "\n",
        "print(\"\\nüìä GPT-4o COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(analysis_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Additional Analysis: Context Comparison\n",
        "\n",
        "Let's also examine the specific contexts each system retrieved to understand the differences in retrieval strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîç CONTEXT RETRIEVAL COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüîπ TRADITIONAL RAG CONTEXT SOURCES:\")\n",
        "print(\"-\" * 40)\n",
        "for i, (text, score) in enumerate(traditional_result['context'], 1):\n",
        "    print(f\"Source {i} (Score: {score:.3f}):\")\n",
        "    print(f\"   {text[:200]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nüîπ KNOWLEDGE GRAPH RAG CONTEXT SOURCES:\")\n",
        "print(\"-\" * 40)\n",
        "for i, (text, score, metadata) in enumerate(kg_result['context'], 1):\n",
        "    category = metadata.get('category_name', 'Unknown')\n",
        "    entities = metadata.get('entities', [])\n",
        "    print(f\"Source {i} (Score: {score:.3f}, Category: {category}):\")\n",
        "    if entities:\n",
        "        print(f\"   Entities: {', '.join(entities[:5])}\")\n",
        "    print(f\"   {text[:200]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate context analysis\n",
        "context_analysis_template = f\"\"\"Please analyze the context retrieval strategies of these two RAG systems based on the sources they selected:\n",
        "\n",
        "Query: {selected_prompt}\n",
        "\n",
        "Traditional RAG Sources (similarity-based):\n",
        "{chr(10).join([f\"Source {i+1} (Score: {score:.3f}): {text[:150]}...\" for i, (text, score) in enumerate(traditional_result['context'])])}\n",
        "\n",
        "Knowledge Graph RAG Sources (entity + similarity-based):\n",
        "{chr(10).join([f\"Source {i+1} (Score: {score:.3f}, Category: {metadata.get('category_name', 'Unknown')}, Entities: {', '.join(metadata.get('entities', [])[:3])}): {text[:150]}...\" for i, (text, score, metadata) in enumerate(kg_result['context'])])}\n",
        "\n",
        "Please compare:\n",
        "1. How different are the retrieved contexts?\n",
        "2. Which approach found more relevant information for this specific query?\n",
        "3. How did entity extraction and categorization influence the Knowledge Graph RAG's choices?\n",
        "4. Are there any important perspectives or information that one system missed?\n",
        "\n",
        "Provide specific insights about the retrieval strategy differences.\"\"\"\n",
        "\n",
        "context_analysis = gpt4o_analyzer.run([{\"role\": \"user\", \"content\": context_analysis_template}])\n",
        "\n",
        "print(\"\\nüìä CONTEXT RETRIEVAL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(context_analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "This comparison analysis demonstrates the practical differences between traditional vector similarity RAG and knowledge graph enhanced RAG approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéØ COMPARISON SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Selected Prompt: {selected_prompt}\")\n",
        "print(f\"Document Context: {document_description}\")\n",
        "print(\"\\n‚úÖ Analysis Complete!\")\n",
        "print(\"üîÑ To run another comparison, restart from Step 1 with a different prompt\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
