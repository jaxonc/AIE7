{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "    wikipedia_tool,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) #change here gpt4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "##### ‚úÖ Answer:\n",
        "The model knows which tool to use by using the provided information in the context which includes, but is not limited to, instructions in the query/prompt itself and the docstrings in the tool definitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages] #Annotated here lets the type remain a list but the messages provide context to the type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b33110>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b33110>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b33110>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b33110>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "##### ‚úÖ Answer:\n",
        "No. A limit can be imposed several ways but one way is by keeping track of and setting a limit on something like number of cycles or runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_hvCUH5clzqxUygLxHA0BMSoE', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 220, 'total_tokens': 246, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt3gwyPsYtWAN9MqGvUNC2X6cneoV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--53017225-52e4-421e-8c57-555265f4ff14-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_hvCUH5clzqxUygLxHA0BMSoE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 220, 'output_tokens': 26, 'total_tokens': 246, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"Adam Lowry - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Adam_Lowry\", \"content\": \"| Awards and achievements | | |\\\\n| --- | --- | --- |\\\\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \\\\\"Daryl K. (Doc) Seaman Trophy\\\\\")  2010 | Succeeded by Colin Smith \\\\\"Colin Smith (ice hockey)\\\\\") |\\\\n| Sporting positions | | |\\\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\\\n\\\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\\\n\\\\n## Personal life [...] Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre \\\\\"Center (ice hockey)\\\\\") and  captain \\\\\"Captain (ice hockey)\\\\\") of the Winnipeg Jets of the National Hockey League (NHL).\\\\n\\\\n## Early life\", \"score\": 0.8896154}, {\"title\": \"Adam Lowry named Jets captain | Winnipeg Jets - NHL.com\", \"url\": \"https://www.nhl.com/jets/news/adam-lowry-named-jets-captain\", \"content\": \"The 30-year-old wore an √¢\\x80\\x9cA√¢\\x80\\x9d for the first time in 2022-23 along with Josh Morrissey and Mark Scheifele who will both serve as alternates for the team this year. The Jets didn√¢\\x80\\x99t have a captain last season as head coach Rick Bowness and his staff wanted more voices in the dressing room.\\\\n\\\\n√¢\\x80\\x9cIt went well last year. Mark, Adam and Josh did a great job in the room which is what we wanted. We all know Adam is the first guy on the ice to stick up for his teammates,√¢\\x80\\x9d said Bowness. [...] √¢\\x80\\x9cHe√¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point it√¢\\x80\\x99s the right time to name Adam as our captain.√¢\\x80\\x9d [...] √¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something I√¢\\x80\\x99m really looking forward too.√¢\\x80\\x9d\", \"score\": 0.85509074}, {\"title\": \"Winnipeg Jets - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Winnipeg_Jets\", \"content\": \"In the 2021‚Äì22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022‚Äì23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023‚Äì24 season, forward Adam Lowry was appointed team captain. The Jets then clinched [...] | 6 | Canada | Colin Miller \\\\\"Colin Miller (ice hockey, born 1992)\\\\\") | D | R | 32 | 2024 | Sault Ste. Marie, Ontario |\\\\n| 44 | Canada | Josh Morrissey (A#Alternate_captains \\\\\"Captain (ice hockey)\\\\\")) | D | L | 30 | 2013 | Calgary, Alberta |\\\\n| 7 | Russia | Vladislav Namestnikov | C \\\\\"Centre (ice hockey)\\\\\") | L | 32 | 2023 | Voskresensk, Russia |\\\\n| 62 | Switzerland | Nino Niederreiter | RW \\\\\"Winger (ice hockey)\\\\\") | L | 32 | 2023 | Chur, Switzerland | [...] |  v  t  e  Winnipeg Jets | |\\\\n| --- | --- |\\\\n|  Formerly the Atlanta Thrashers  Founded in 1999  Based in Winnipeg, Manitoba | |\\\\n| Franchise |  Team  General managers  Coaches  Players  Captains  Draft picks   + expansion draft  Seasons  Current season |\\\\n| History |  Records  Award winners  Broadcasters |\\\\n| Personnel | Owner(s)  True North Sports & Entertainment (Mark Chipman, chairman)  General manager  Kevin Cheveldayoff  Head coach  Scott Arniel  Team captain  Adam Lowry  Current roster |\", \"score\": 0.84027237}, {\"title\": \"Team Captains of Winnipeg Jets - Elite Prospects\", \"url\": \"https://www.eliteprospects.com/team/9966/winnipeg-jets/captaincy-history\", \"content\": \"| Season | League | ‚ÄúC‚Äù Captain(s) | ‚ÄúA‚Äù Alternate Captain(s) |\\\\n| --- | --- | --- | --- |\\\\n| 2025-2026  2025-26 | NHL | Adam Lowry |  |\\\\n| 2024-2025  2024-25 | NHL | Adam Lowry | Josh Morrissey  Neal Pionk  Mark Scheifele |\\\\n| 2023-2024  2023-24 | NHL | Adam Lowry | Josh Morrissey  Mark Scheifele |\\\\n| 2022-2023  2022-23 | NHL |  | Adam Lowry  Josh Morrissey  Mark Scheifele |\\\\n| 2021-2022  2021-22 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele | [...] | Name | League |\\\\n| --- | --- |\\\\n| Canada Chase Stillman (F) | AHL |\\\\n| USA Kieffer Bellows (F) | SHL |\\\\n| Latvia Arturs Silovs (G) | NHL |\\\\n| Canada Riley Stillman (D) | AHL |\\\\n| Canada Gavin McKenna (F) | NCAA |\\\\n| Canada Cory Stillman (F) | NHL |\\\\n| USA Joe Pavelski (F) | NHL |\\\\n| Canada Brian Bellows (F) | NHL |\\\\n| Canada Karl Boudrias (D) | ECHL |\\\\n| Czechia Simon Katolicky (F) | U20 SM-sarja |\\\\n| Canada Josh Slegers (D) | FPHL |\\\\n| Canada Fernando Zegarra (D) | GOJHL | [...] Canada\\\\nUSA\\\\nLatvia\\\\nCanada\\\\nCanada\\\\nCanada\\\\nUSA\\\\nCanada\\\\nCanada\\\\nCzechia\\\\nCanada\\\\nCanada\\\\nCzechia\\\\nCanada\\\\nUSA\\\\nCanada\\\\nUSA\\\\nCanada\\\\nUSA\\\\nUSA\\\\n\\\\n##### Scoring Leaders\\\\n\\\\n| # | Player | GP | G | A | TP |\\\\n| --- | --- | --- | --- | --- | --- |\\\\n| 1. | Tanner Hopps | 18 | 20 | 32 | 52 |\\\\n| 2. | Yu Hikosaka | 20 | 26 | 23 | 49 |\\\\n| 3. | Scott Timmins | 18 | 16 | 31 | 47 |\\\\n| 4. | Joakim Erdugan | 21 | 13 | 33 | 46 |\\\\n| 5. | Kolton Shindle | 18 | 25 | 20 | 45 |\\\\n\\\\n##### Popular League Pages\\\\n\\\\n##### NHL Rosters\\\\n\\\\n##### AHL Rosters\", \"score\": 0.7803451}, {\"title\": \"Lowry named Jets captain, replaces Wheeler | NHL.com\", \"url\": \"https://www.nhl.com/news/adam-lowry-named-winnipeg-captain\", \"content\": \"Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] NHL logo\\\\nNHL logo\\\\n\\\\n# Lowry named Jets captain, replaces Wheeler\\\\n\\\\n30-year-old forward entering 10th season with Winnipeg\\\\n\\\\nLowry_Jets\\\\n\\\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\\\n\\\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] \\'OK, Adam\\'s our captain.\\' That wasn\\'t the case at all. We had a lot of conversations over the summer with the staff, with management and everyone involved. So, ultimately we came to this conclusion.\\\\\"\", \"score\": 0.6783488}]', name='tavily_search_results_json', id='36ce59b3-c405-43c4-9bc0-65fa01e671de', tool_call_id='call_hvCUH5clzqxUygLxHA0BMSoE', artifact={'query': 'current captain of the Winnipeg Jets 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Adam_Lowry', 'title': 'Adam Lowry - Wikipedia', 'content': '| Awards and achievements | | |\\n| --- | --- | --- |\\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \"Daryl K. (Doc) Seaman Trophy\")  2010 | Succeeded by Colin Smith \"Colin Smith (ice hockey)\") |\\n| Sporting positions | | |\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\n\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\n\\n## Personal life [...] Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre \"Center (ice hockey)\") and  captain \"Captain (ice hockey)\") of the Winnipeg Jets of the National Hockey League (NHL).\\n\\n## Early life', 'score': 0.8896154, 'raw_content': None}, {'url': 'https://www.nhl.com/jets/news/adam-lowry-named-jets-captain', 'title': 'Adam Lowry named Jets captain | Winnipeg Jets - NHL.com', 'content': 'The 30-year-old wore an √¢\\x80\\x9cA√¢\\x80\\x9d for the first time in 2022-23 along with Josh Morrissey and Mark Scheifele who will both serve as alternates for the team this year. The Jets didn√¢\\x80\\x99t have a captain last season as head coach Rick Bowness and his staff wanted more voices in the dressing room.\\n\\n√¢\\x80\\x9cIt went well last year. Mark, Adam and Josh did a great job in the room which is what we wanted. We all know Adam is the first guy on the ice to stick up for his teammates,√¢\\x80\\x9d said Bowness. [...] √¢\\x80\\x9cHe√¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point it√¢\\x80\\x99s the right time to name Adam as our captain.√¢\\x80\\x9d [...] √¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something I√¢\\x80\\x99m really looking forward too.√¢\\x80\\x9d', 'score': 0.85509074, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Winnipeg_Jets', 'title': 'Winnipeg Jets - Wikipedia', 'content': 'In the 2021‚Äì22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022‚Äì23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023‚Äì24 season, forward Adam Lowry was appointed team captain. The Jets then clinched [...] | 6 | Canada | Colin Miller \"Colin Miller (ice hockey, born 1992)\") | D | R | 32 | 2024 | Sault Ste. Marie, Ontario |\\n| 44 | Canada | Josh Morrissey (A#Alternate_captains \"Captain (ice hockey)\")) | D | L | 30 | 2013 | Calgary, Alberta |\\n| 7 | Russia | Vladislav Namestnikov | C \"Centre (ice hockey)\") | L | 32 | 2023 | Voskresensk, Russia |\\n| 62 | Switzerland | Nino Niederreiter | RW \"Winger (ice hockey)\") | L | 32 | 2023 | Chur, Switzerland | [...] |  v  t  e  Winnipeg Jets | |\\n| --- | --- |\\n|  Formerly the Atlanta Thrashers  Founded in 1999  Based in Winnipeg, Manitoba | |\\n| Franchise |  Team  General managers  Coaches  Players  Captains  Draft picks   + expansion draft  Seasons  Current season |\\n| History |  Records  Award winners  Broadcasters |\\n| Personnel | Owner(s)  True North Sports & Entertainment (Mark Chipman, chairman)  General manager  Kevin Cheveldayoff  Head coach  Scott Arniel  Team captain  Adam Lowry  Current roster |', 'score': 0.84027237, 'raw_content': None}, {'url': 'https://www.eliteprospects.com/team/9966/winnipeg-jets/captaincy-history', 'title': 'Team Captains of Winnipeg Jets - Elite Prospects', 'content': '| Season | League | ‚ÄúC‚Äù Captain(s) | ‚ÄúA‚Äù Alternate Captain(s) |\\n| --- | --- | --- | --- |\\n| 2025-2026  2025-26 | NHL | Adam Lowry |  |\\n| 2024-2025  2024-25 | NHL | Adam Lowry | Josh Morrissey  Neal Pionk  Mark Scheifele |\\n| 2023-2024  2023-24 | NHL | Adam Lowry | Josh Morrissey  Mark Scheifele |\\n| 2022-2023  2022-23 | NHL |  | Adam Lowry  Josh Morrissey  Mark Scheifele |\\n| 2021-2022  2021-22 | NHL | Blake Wheeler | Josh Morrissey  Mark Scheifele | [...] | Name | League |\\n| --- | --- |\\n| Canada Chase Stillman (F) | AHL |\\n| USA Kieffer Bellows (F) | SHL |\\n| Latvia Arturs Silovs (G) | NHL |\\n| Canada Riley Stillman (D) | AHL |\\n| Canada Gavin McKenna (F) | NCAA |\\n| Canada Cory Stillman (F) | NHL |\\n| USA Joe Pavelski (F) | NHL |\\n| Canada Brian Bellows (F) | NHL |\\n| Canada Karl Boudrias (D) | ECHL |\\n| Czechia Simon Katolicky (F) | U20 SM-sarja |\\n| Canada Josh Slegers (D) | FPHL |\\n| Canada Fernando Zegarra (D) | GOJHL | [...] Canada\\nUSA\\nLatvia\\nCanada\\nCanada\\nCanada\\nUSA\\nCanada\\nCanada\\nCzechia\\nCanada\\nCanada\\nCzechia\\nCanada\\nUSA\\nCanada\\nUSA\\nCanada\\nUSA\\nUSA\\n\\n##### Scoring Leaders\\n\\n| # | Player | GP | G | A | TP |\\n| --- | --- | --- | --- | --- | --- |\\n| 1. | Tanner Hopps | 18 | 20 | 32 | 52 |\\n| 2. | Yu Hikosaka | 20 | 26 | 23 | 49 |\\n| 3. | Scott Timmins | 18 | 16 | 31 | 47 |\\n| 4. | Joakim Erdugan | 21 | 13 | 33 | 46 |\\n| 5. | Kolton Shindle | 18 | 25 | 20 | 45 |\\n\\n##### Popular League Pages\\n\\n##### NHL Rosters\\n\\n##### AHL Rosters', 'score': 0.7803451, 'raw_content': None}, {'url': 'https://www.nhl.com/news/adam-lowry-named-winnipeg-captain', 'title': 'Lowry named Jets captain, replaces Wheeler | NHL.com', 'content': 'Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] NHL logo\\nNHL logo\\n\\n# Lowry named Jets captain, replaces Wheeler\\n\\n30-year-old forward entering 10th season with Winnipeg\\n\\nLowry_Jets\\n\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\n\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] \\'OK, Adam\\'s our captain.\\' That wasn\\'t the case at all. We had a lot of conversations over the summer with the staff, with management and everyone involved. So, ultimately we came to this conclusion.\"', 'score': 0.6783488, 'raw_content': None}], 'response_time': 4.44})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry. He was named captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 2206, 'total_tokens': 2232, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt3h2FiWHs8yvNufPiOZXJRzUymZH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ddd43d4e-5cd8-4e9a-a3e3-5f8f24ab584c-0', usage_metadata={'input_tokens': 2206, 'output_tokens': 26, 'total_tokens': 2232, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5F02zl2WgG2HK2NBwXuyTVt7', 'function': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 236, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt3io59lFwM7tCwGkZpFFtTSepTQz', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9a674473-e8ad-42f7-ad85-72b4aa56faa4-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_5F02zl2WgG2HK2NBwXuyTVt7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 236, 'output_tokens': 16, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='863043ca-97e8-4ec9-b6e2-de0d57110738', tool_call_id='call_5F02zl2WgG2HK2NBwXuyTVt7')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sSGEkACK5htLRroQmAvSLoxw', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_TReQ4nVCPRYTVhRrCMJsXlfz', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_SMFZrqwNGj6QIybbi6VBWSdu', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_lkcmLQbwkTkAvtwgBQqtFQrP', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 1206, 'total_tokens': 1318, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt3iqvpshC9xLk4lHS7kQfYfhJ0Nw', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e9721e49-10ee-466c-84bf-9ecb1d3105ed-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers latest tweet'}, 'id': 'call_sSGEkACK5htLRroQmAvSLoxw', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Artidoro Pagnoni latest tweet'}, 'id': 'call_TReQ4nVCPRYTVhRrCMJsXlfz', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Ari Holtzman latest tweet'}, 'id': 'call_SMFZrqwNGj6QIybbi6VBWSdu', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Luke Zettlemoyer latest tweet'}, 'id': 'call_lkcmLQbwkTkAvtwgBQqtFQrP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1206, 'output_tokens': 112, 'total_tokens': 1318, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: tavily_search_results_json\n",
            "[ToolMessage(content='[{\"title\": \"Tim Dettmers ‚Äî Making deep learning accessible.\", \"url\": \"https://timdettmers.com/\", \"content\": \"Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\n## TPUs vs GPUs for Transformers (BERT)\\\\n\\\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\\\n\\\\n## LLM.int8() and Emergent Features\\\\n\\\\n2022-08-17 by Tim Dettmers 13 Comments [...] From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job.\\\\n\\\\nFiled Under: Deep Learning Tagged With: emergent features, LLM.int8()\\\\n\\\\n## How to Choose Your Grad School\\\\n\\\\n2022-03-13 by Tim Dettmers 18 Comments\", \"score\": 0.6554468}, {\"title\": \"PhD Archives - Tim Dettmers\", \"url\": \"https://timdettmers.com/tag/phd/\", \"content\": \"Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\n## Credit Assignment in Deep Learning\\\\n\\\\n2017-09-16 by Tim Dettmers 15 Comments [...] [[Read more‚Ä¶] about How to Choose Your Grad School](\\\\n\\\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\n## On Creativity in Academia\\\\n\\\\n2019-09-03 by Tim Dettmers 5 Comments [...] [[Read more‚Ä¶] about On Creativity in Academia](\\\\n\\\\nFiled Under: Academia, PhD Life, Science Tagged With: PhD\\\\n\\\\n## Machine Learning PhD Applications ‚Äî Everything You Need to Know\\\\n\\\\n2018-11-26 by Tim Dettmers 154 Comments\", \"score\": 0.54710174}, {\"title\": \"Tim Dettmers TimDettmers - GitHub\", \"url\": \"https://github.com/timdettmers\", \"content\": \"Contact GitHub support about this user‚Äôs behavior.\\\\nLearn more about reporting abuse.\\\\n\\\\n## Pinned Loading\\\\n\\\\nConvolutional 2D Knowledge Graph Embeddings resources\\\\n\\\\nPython\\\\n680\\\\n165\\\\n\\\\nSparse learning library and sparse momentum resources.\\\\n\\\\nPython\\\\n381\\\\n45\\\\n\\\\nForked from kimiyoung/transformer-xl\\\\n\\\\nPython\\\\n64\\\\n3\\\\n\\\\nResources to calculate how to become a carbon neutral lab.\\\\n\\\\nPython\\\\n61\\\\n5\\\\n\\\\nDeep neural network framework for multiple GPUs\\\\n\\\\nCuda\\\\n33\\\\n15 [...] Achievement: Starstruck\\\\nAchievement: Pair Extraordinaire\\\\nAchievement: Arctic Code Vault Contributor\\\\nAchievement: Pull Shark\\\\n\\\\n## Achievements\\\\n\\\\nAchievement: Starstruck\\\\nAchievement: Pair Extraordinaire\\\\nAchievement: Arctic Code Vault Contributor\\\\nAchievement: Pull Shark\\\\n\\\\n# Block or report TimDettmers\\\\n\\\\nPrevent this user from interacting with your repositories and sending you notifications.\\\\nLearn more about blocking users.\\\\n\\\\nYou must be logged in to block users. [...] ## Navigation Menu\\\\n\\\\n# Search code, repositories, users, issues, pull requests...\\\\n\\\\n# Provide feedback\\\\n\\\\nWe read every piece of feedback, and take your input very seriously.\\\\n\\\\n# Saved searches\\\\n\\\\n## Use saved searches to filter your results more quickly\\\\n\\\\nTo see all available qualifiers, see our documentation.\\\\n\\\\n@TimDettmers\\\\n@TimDettmers\\\\nView TimDettmers\\'s full-sized avatar\\\\n\\\\n# Tim Dettmers TimDettmers\\\\n\\\\n## Achievements\", \"score\": 0.4547875}, {\"title\": \"About Me - Tim Dettmers\", \"url\": \"https://timdettmers.com/about/\", \"content\": \"2021                 NeurIPS 2021 Best Reviewer Award\\\\n\\\\n2018/2019      Jeff Dean ‚Äì Heidi Hopper Endowed Regental Fellowship\\\\n\\\\n2016/2017      Google Scholarship\\\\n\\\\n## Service\\\\n\\\\nReviewing:\\\\n\\\\n## Primary Sidebar\\\\n\\\\n### What to read next:\\\\n\\\\n### Recent Posts\\\\n\\\\n## Secondary Sidebar\\\\n\\\\nCopyright ¬© 2024 ¬∑ Genesis Framework ¬∑ WordPress ¬∑ Log in\\\\n\\\\n## \\\\n\\\\n## \\\\n\\\\n### [...] Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. [arXiv] [bib] [code] [data] [Q&A]\\\\n\\\\n#### 2016\\\\n\\\\n8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016. [arXiv] [bib] [code] [data]\\\\n\\\\n## Awards & Honors\\\\n\\\\n2023                 Madrona Prize\\\\n\\\\n2023                 Google Open Source Award\\\\n\\\\n2023                 PyTorch Foundation Award\\\\n\\\\n2023                 Martin & Beate Block Award [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my\", \"score\": 0.42849702}, {\"title\": \"Author: Tim Dettmers | NVIDIA Technical Blog\", \"url\": \"https://developer.nvidia.com/blog/author/tdettmers/\", \"content\": \"Figure 1: Value iteration constructs the value function over all states over time. Here each square is a state: S is the start state, G the goal state, T squares are traps, and black squares cannot be entered. In value iteration we initialize the rewards (traps and goal state) and then these reward values spread over time until an equilibrium is reached. Depending on the penalty value on traps and the reward value for the goal different solution patterns might emerge; the last two grids show [...] ### Deep Learning in a Nutshell: Reinforcement Learning\\\\n\\\\nCUDA AI Cube\\\\nCUDA AI Cube\\\\n\\\\n### Deep Learning in a Nutshell: Sequence Learning\\\\n\\\\n### Deep Learning in a Nutshell: History and Training\\\\n\\\\n### Deep Learning in a Nutshell: Core Concepts [...] such solution states.\", \"score\": 0.42448413}]', name='tavily_search_results_json', id='bf79e684-ab1d-45c0-b592-c6dd5836e57a', tool_call_id='call_sSGEkACK5htLRroQmAvSLoxw', artifact={'query': 'Tim Dettmers latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://timdettmers.com/', 'title': 'Tim Dettmers ‚Äî Making deep learning accessible.', 'content': 'Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\n## TPUs vs GPUs for Transformers (BERT)\\n\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\n\\n## LLM.int8() and Emergent Features\\n\\n2022-08-17 by Tim Dettmers 13 Comments [...] From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job.\\n\\nFiled Under: Deep Learning Tagged With: emergent features, LLM.int8()\\n\\n## How to Choose Your Grad School\\n\\n2022-03-13 by Tim Dettmers 18 Comments', 'score': 0.6554468, 'raw_content': None}, {'url': 'https://timdettmers.com/tag/phd/', 'title': 'PhD Archives - Tim Dettmers', 'content': 'Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\n## Credit Assignment in Deep Learning\\n\\n2017-09-16 by Tim Dettmers 15 Comments [...] [[Read more‚Ä¶] about How to Choose Your Grad School](\\n\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\n## On Creativity in Academia\\n\\n2019-09-03 by Tim Dettmers 5 Comments [...] [[Read more‚Ä¶] about On Creativity in Academia](\\n\\nFiled Under: Academia, PhD Life, Science Tagged With: PhD\\n\\n## Machine Learning PhD Applications ‚Äî Everything You Need to Know\\n\\n2018-11-26 by Tim Dettmers 154 Comments', 'score': 0.54710174, 'raw_content': None}, {'url': 'https://github.com/timdettmers', 'title': 'Tim Dettmers TimDettmers - GitHub', 'content': \"Contact GitHub support about this user‚Äôs behavior.\\nLearn more about reporting abuse.\\n\\n## Pinned Loading\\n\\nConvolutional 2D Knowledge Graph Embeddings resources\\n\\nPython\\n680\\n165\\n\\nSparse learning library and sparse momentum resources.\\n\\nPython\\n381\\n45\\n\\nForked from kimiyoung/transformer-xl\\n\\nPython\\n64\\n3\\n\\nResources to calculate how to become a carbon neutral lab.\\n\\nPython\\n61\\n5\\n\\nDeep neural network framework for multiple GPUs\\n\\nCuda\\n33\\n15 [...] Achievement: Starstruck\\nAchievement: Pair Extraordinaire\\nAchievement: Arctic Code Vault Contributor\\nAchievement: Pull Shark\\n\\n## Achievements\\n\\nAchievement: Starstruck\\nAchievement: Pair Extraordinaire\\nAchievement: Arctic Code Vault Contributor\\nAchievement: Pull Shark\\n\\n# Block or report TimDettmers\\n\\nPrevent this user from interacting with your repositories and sending you notifications.\\nLearn more about blocking users.\\n\\nYou must be logged in to block users. [...] ## Navigation Menu\\n\\n# Search code, repositories, users, issues, pull requests...\\n\\n# Provide feedback\\n\\nWe read every piece of feedback, and take your input very seriously.\\n\\n# Saved searches\\n\\n## Use saved searches to filter your results more quickly\\n\\nTo see all available qualifiers, see our documentation.\\n\\n@TimDettmers\\n@TimDettmers\\nView TimDettmers's full-sized avatar\\n\\n# Tim Dettmers TimDettmers\\n\\n## Achievements\", 'score': 0.4547875, 'raw_content': None}, {'url': 'https://timdettmers.com/about/', 'title': 'About Me - Tim Dettmers', 'content': '2021                 NeurIPS 2021 Best Reviewer Award\\n\\n2018/2019      Jeff Dean ‚Äì Heidi Hopper Endowed Regental Fellowship\\n\\n2016/2017      Google Scholarship\\n\\n## Service\\n\\nReviewing:\\n\\n## Primary Sidebar\\n\\n### What to read next:\\n\\n### Recent Posts\\n\\n## Secondary Sidebar\\n\\nCopyright ¬© 2024 ¬∑ Genesis Framework ¬∑ WordPress ¬∑ Log in\\n\\n## \\n\\n## \\n\\n### [...] Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. [arXiv] [bib] [code] [data] [Q&A]\\n\\n#### 2016\\n\\n8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016. [arXiv] [bib] [code] [data]\\n\\n## Awards & Honors\\n\\n2023                 Madrona Prize\\n\\n2023                 Google Open Source Award\\n\\n2023                 PyTorch Foundation Award\\n\\n2023                 Martin & Beate Block Award [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my', 'score': 0.42849702, 'raw_content': None}, {'url': 'https://developer.nvidia.com/blog/author/tdettmers/', 'title': 'Author: Tim Dettmers | NVIDIA Technical Blog', 'content': 'Figure 1: Value iteration constructs the value function over all states over time. Here each square is a state: S is the start state, G the goal state, T squares are traps, and black squares cannot be entered. In value iteration we initialize the rewards (traps and goal state) and then these reward values spread over time until an equilibrium is reached. Depending on the penalty value on traps and the reward value for the goal different solution patterns might emerge; the last two grids show [...] ### Deep Learning in a Nutshell: Reinforcement Learning\\n\\nCUDA AI Cube\\nCUDA AI Cube\\n\\n### Deep Learning in a Nutshell: Sequence Learning\\n\\n### Deep Learning in a Nutshell: History and Training\\n\\n### Deep Learning in a Nutshell: Core Concepts [...] such solution states.', 'score': 0.42448413, 'raw_content': None}], 'response_time': 3.53}), ToolMessage(content='[{\"title\": \"Artidoro Pagnoni: Ciao!\", \"url\": \"https://artidoro.github.io/\", \"content\": \"Artidoro Pagnoni\\\\n\\\\n### Artidoro Pagnoni\\\\n\\\\nPhD student in NLP at the University of Washington\\\\n\\\\n# Ciao!\\\\n\\\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends. [...] I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models‚Äô reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use.\", \"score\": 0.6490677}, {\"title\": \"Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X\", \"url\": \"https://twitter.com/ArtidoroPagnoni/highlights\", \"content\": \"Artidoro Pagnoni\\'s Highlights ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead\", \"score\": 0.64719695}, {\"title\": \"Artidoro Pagnoni (@ApagnoniPagnoni) / X\", \"url\": \"https://twitter.com/apagnonipagnoni?lang=ms\", \"content\": \"Siaran Artidoro Pagnoni. Artidoro Pagnoni ... My favorite shot from the last rocket landing attempt on the droneship. Imej.\", \"score\": 0.6102915}, {\"title\": \"Artidoro Pagnoni (@ArtidoroPagnoni) / X\", \"url\": \"https://twitter.com/ArtidoroPagnoni\", \"content\": \"Artidoro Pagnoni\\'s posts ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead of\", \"score\": 0.6100127}, {\"title\": \"Artidoro Pagnoni - Hugging Face\", \"url\": \"https://huggingface.co/artidoro/activity/all\", \"content\": \"Hugging Face\\'s logo\\\\nArtidoro Pagnoni\\'s picture\\\\n\\\\n# Artidoro Pagnoni\\\\n\\\\n### AI & ML interests\\\\n\\\\n### Recent Activity\\\\n\\\\n### Organizations\\\\n\\\\nUniversity of Washington NLP\\'s profile picture\\\\n\\\\n## artidoro\\'s activity\\\\n\\\\n#### allenai/DataDecide-ppl-results\\\\n\\\\n#### facebook/blt\\\\n\\\\n#### SuperBPE: Space Travel for Language Models\\\\n\\\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\\\n\\\\n#### Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization [...] #### Byte Latent Transformer: Patches Scale Better Than Tokens\\\\n\\\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\\\n\\\\n#### artidoro/model-tvergho\\\\n\\\\n#### artidoro/model-vinaic\\\\n\\\\n#### artidoro/model-vinaia\\\\n\\\\n#### meta-llama/Llama-2-7b\\\\n\\\\n#### Error trying to submit LLaMA 2 base model\\\\n\\\\n#### uwnlp/llama-2-70b-qlora-openorca\\\\n\\\\n#### TheBloke/llama-2-70b-Guanaco-QLoRA-fp16\\\\n\\\\n#### timdettmers/guanaco-33b\\\\n\\\\n#### did it stop working?\\\\n\\\\n#### How to fine-tune the Guanaco (7B, 13B) model? [...] #### timdettmers/openassistant-guanaco\\\\n\\\\n#### Guanaco-13B\", \"score\": 0.34277}]', name='tavily_search_results_json', id='0564fdaa-0cc6-441c-ab19-15028c76c2bf', tool_call_id='call_TReQ4nVCPRYTVhRrCMJsXlfz', artifact={'query': 'Artidoro Pagnoni latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://artidoro.github.io/', 'title': 'Artidoro Pagnoni: Ciao!', 'content': 'Artidoro Pagnoni\\n\\n### Artidoro Pagnoni\\n\\nPhD student in NLP at the University of Washington\\n\\n# Ciao!\\n\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends. [...] I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models‚Äô reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use.', 'score': 0.6490677, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni/highlights', 'title': 'Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's Highlights ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead\", 'score': 0.64719695, 'raw_content': None}, {'url': 'https://twitter.com/apagnonipagnoni?lang=ms', 'title': 'Artidoro Pagnoni (@ApagnoniPagnoni) / X', 'content': 'Siaran Artidoro Pagnoni. Artidoro Pagnoni ... My favorite shot from the last rocket landing attempt on the droneship. Imej.', 'score': 0.6102915, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni', 'title': 'Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's posts ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead of\", 'score': 0.6100127, 'raw_content': None}, {'url': 'https://huggingface.co/artidoro/activity/all', 'title': 'Artidoro Pagnoni - Hugging Face', 'content': \"Hugging Face's logo\\nArtidoro Pagnoni's picture\\n\\n# Artidoro Pagnoni\\n\\n### AI & ML interests\\n\\n### Recent Activity\\n\\n### Organizations\\n\\nUniversity of Washington NLP's profile picture\\n\\n## artidoro's activity\\n\\n#### allenai/DataDecide-ppl-results\\n\\n#### facebook/blt\\n\\n#### SuperBPE: Space Travel for Language Models\\n\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\n\\n#### Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization [...] #### Byte Latent Transformer: Patches Scale Better Than Tokens\\n\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\n\\n#### artidoro/model-tvergho\\n\\n#### artidoro/model-vinaic\\n\\n#### artidoro/model-vinaia\\n\\n#### meta-llama/Llama-2-7b\\n\\n#### Error trying to submit LLaMA 2 base model\\n\\n#### uwnlp/llama-2-70b-qlora-openorca\\n\\n#### TheBloke/llama-2-70b-Guanaco-QLoRA-fp16\\n\\n#### timdettmers/guanaco-33b\\n\\n#### did it stop working?\\n\\n#### How to fine-tune the Guanaco (7B, 13B) model? [...] #### timdettmers/openassistant-guanaco\\n\\n#### Guanaco-13B\", 'score': 0.34277, 'raw_content': None}], 'response_time': 2.91}), ToolMessage(content='[{\"title\": \"Ari Holtzman on X: \\\\\"join me https://t.co/ZRWgwxTQwl\\\\\" / X\", \"url\": \"https://twitter.com/universeinanegg/status/1864053976729002080\", \"content\": \"Log in ¬∑ Sign up. Conversation. Ari Holtzman ¬∑ @universeinanegg. join me. Image. 9:07 PM ¬∑ Dec 3, 2024. ¬∑. 1,216. Views.\", \"score\": 0.72395444}, {\"title\": \"Ari Holtzman on X: \\\\\"Was such a blast to write this with ...\", \"url\": \"https://twitter.com/universeinanegg/status/1943036113183998086\", \"content\": \"@ChenhaoTan ! And thank you to. @PeterWestTM. for many useful conversations as we were putting the argument together! 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 65. Views.\", \"score\": 0.6337056}, {\"title\": \"Ari Holtzman ‚Äì Department of Computer Science\", \"url\": \"https://cs.uchicago.edu/people/ari-holtzman/\", \"content\": \"### Research\\\\n\\\\n### Systems, Architecture & Networking\\\\n\\\\n### Awards & Honors\\\\n\\\\n### Get Updates\\\\n\\\\n### Follow [...] ### Diversity @ UChicago CS\\\\n\\\\nAt UChicago CS, we welcome students of all backgrounds and identities.\\\\n\\\\n### Our BPC Plan\\\\n\\\\nFostering an inclusive environment where students from all backgrounds can achieve their highest potential.\\\\n\\\\n### News\\\\n\\\\n### Report from GlobusWorld 2025: Going Beyond Data\\\\n\\\\nheadshots\\\\n\\\\n### University of Chicago PhD Graduates Secure Tenure-Track Faculty Positions Amid a Competitive Job Market\\\\n\\\\ntext to 3d example [...] ### Democratizing Digital Graphics: An Undergrad‚Äôs Unlikely Path To Putting Agency of 3D-Generation in Users‚Äô Hands\\\\n\\\\n### Events\\\\n\\\\n### Video\\\\n\\\\nfuture of AI panelists\\\\n\\\\n### The Future of AI Panel: Alumni Weekend\\\\n\\\\n### Can we authenticate human creativity?\\\\n\\\\nheadshot\\\\n\\\\n### AI and the Future of Work Panel: Featuring Nick Feamster\\\\n\\\\nheadhsot\\\\nheadhsot\\\\n\\\\n# Ari Holtzman\", \"score\": 0.38699567}, {\"title\": \"Ari Holtzman on X\", \"url\": \"https://twitter.com/universeinanegg/status/1778970275897590098\", \"content\": \"I wonder what it would take to make a chat model that wasn\\'t so on-the-nose with every response. A \\\\\"show don\\'t tell\\\\\" model.\", \"score\": 0.31744793}, {\"title\": \"Ari Holtzman\", \"url\": \"https://scholar.google.com/citations?user=jtrAdywAAAAJ&hl=en\", \"content\": \"Assistant Professor at University Chicago - \\u202a\\u202aCited by 18168\\u202c\\u202c - \\u202aCommunication & Intelligence\\u202c - \\u202aGenerative Models\\u202c - \\u202aConceptualization\\u202c\", \"score\": 0.22663853}]', name='tavily_search_results_json', id='2ae83d46-3f78-48f2-aa68-9a767fcf5781', tool_call_id='call_SMFZrqwNGj6QIybbi6VBWSdu', artifact={'query': 'Ari Holtzman latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://twitter.com/universeinanegg/status/1864053976729002080', 'title': 'Ari Holtzman on X: \"join me https://t.co/ZRWgwxTQwl\" / X', 'content': 'Log in ¬∑ Sign up. Conversation. Ari Holtzman ¬∑ @universeinanegg. join me. Image. 9:07 PM ¬∑ Dec 3, 2024. ¬∑. 1,216. Views.', 'score': 0.72395444, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1943036113183998086', 'title': 'Ari Holtzman on X: \"Was such a blast to write this with ...', 'content': '@ChenhaoTan ! And thank you to. @PeterWestTM. for many useful conversations as we were putting the argument together! 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 65. Views.', 'score': 0.6337056, 'raw_content': None}, {'url': 'https://cs.uchicago.edu/people/ari-holtzman/', 'title': 'Ari Holtzman ‚Äì Department of Computer Science', 'content': '### Research\\n\\n### Systems, Architecture & Networking\\n\\n### Awards & Honors\\n\\n### Get Updates\\n\\n### Follow [...] ### Diversity @ UChicago CS\\n\\nAt UChicago CS, we welcome students of all backgrounds and identities.\\n\\n### Our BPC Plan\\n\\nFostering an inclusive environment where students from all backgrounds can achieve their highest potential.\\n\\n### News\\n\\n### Report from GlobusWorld 2025: Going Beyond Data\\n\\nheadshots\\n\\n### University of Chicago PhD Graduates Secure Tenure-Track Faculty Positions Amid a Competitive Job Market\\n\\ntext to 3d example [...] ### Democratizing Digital Graphics: An Undergrad‚Äôs Unlikely Path To Putting Agency of 3D-Generation in Users‚Äô Hands\\n\\n### Events\\n\\n### Video\\n\\nfuture of AI panelists\\n\\n### The Future of AI Panel: Alumni Weekend\\n\\n### Can we authenticate human creativity?\\n\\nheadshot\\n\\n### AI and the Future of Work Panel: Featuring Nick Feamster\\n\\nheadhsot\\nheadhsot\\n\\n# Ari Holtzman', 'score': 0.38699567, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1778970275897590098', 'title': 'Ari Holtzman on X', 'content': 'I wonder what it would take to make a chat model that wasn\\'t so on-the-nose with every response. A \"show don\\'t tell\" model.', 'score': 0.31744793, 'raw_content': None}, {'url': 'https://scholar.google.com/citations?user=jtrAdywAAAAJ&hl=en', 'title': 'Ari Holtzman', 'content': 'Assistant Professor at University Chicago - \\u202a\\u202aCited by 18168\\u202c\\u202c - \\u202aCommunication & Intelligence\\u202c - \\u202aGenerative Models\\u202c - \\u202aConceptualization\\u202c', 'score': 0.22663853, 'raw_content': None}], 'response_time': 10.01}), ToolMessage(content='[{\"title\": \"Luke Zettlemoyer (@LukeZettlemoyer) / X\", \"url\": \"https://x.com/lukezettlemoyer?lang=en\", \"content\": \"Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.. Embedded video. 0:34.\", \"score\": 0.6188962}, {\"title\": \"Luke Zettlemoyer - University of Washington\", \"url\": \"https://homes.cs.washington.edu/~lsz/\", \"content\": \"Jun 6, 2025: I gave a talk on mixed-modal LMs at the Frontiers in NeuroAI workshop at the Kempner Institute. Feb 11, 2025: Congratulations to Chunting and Lili and the Tranfusion team for getting an oral at ICLR 2025! Dec 3, 2024: Elected ACL President (2025 VP Elect; 2026 VP; 2027 Pres; 2028 Past Pres) Aug 11, 2024: Congrats to OLMo team for winning a Best Theme Paper Award at ACL 2024! Aug 11, 2024: Congrats to Dolma team for winning a Best Resource Paper Award at ACL 2024! Jan 15, 2024: [...] Congratulations to Xian and team for getting oral at ICLR 2024 for their work on instruction backtranslation! Oct 10, 2023: I was on the twiml podcast, Episode 650 Sep 21, 2023: Congrauations to Tim and Arti for the QLoRA paper getting an oral at NeurIPS 2023! Sep 21, 2023: Congrauations to Timo and team on the Toolformer paper getting an oral at NeurIPS 2023! Jul 24, 2023: I was on The Thesis Review with Sean Welleck Episode 45 Jan 6, 2022: I was named an ACL Fellow! [...] ACL Fellow (2021), as well as winning a PECASE award (2016), an Allen Distinguished Investigator award (2014), and many best paper awards. I was an undergraduate at NC State, recieved my PhD from MIT, and was a postdocal researcher at the University of Edinburgh.\", \"score\": 0.6161283}, {\"title\": \"Luke Zettlemoyer - X\", \"url\": \"https://x.com/LukeZettlemoyer/status/1003662931479941120?lang=ar-x-fm\", \"content\": \"Luke Zettlemoyer ¬∑ @LukeZettlemoyer. Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT\", \"score\": 0.5533369}, {\"title\": \"Posts with replies by Luke Zettlemoyer (@LukeZettlemoyer) / X\", \"url\": \"https://twitter.com/LukeZettlemoyer/with_replies\", \"content\": \"Joined September 2015. 2,120 Following ¬∑ 9,556 Followers ¬∑ Posts ¬∑ Replies ¬∑ Media. @LukeZettlemoyer hasn\\'t posted. When they do, their posts will show up\", \"score\": 0.37855154}, {\"title\": \"\\u202aLuke Zettlemoyer\\u202c - \\u202aGoogle Scholar\\u202c\", \"url\": \"https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=en\", \"content\": \"New articles by this author. New citations to this author. New articles related to this author\\'s research. Email address for updates ... Luke Zettlemoyer.\", \"score\": 0.32309222}]', name='tavily_search_results_json', id='a806fc29-a9c5-4883-9845-3f5553e704b7', tool_call_id='call_lkcmLQbwkTkAvtwgBQqtFQrP', artifact={'query': 'Luke Zettlemoyer latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/lukezettlemoyer?lang=en', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.. Embedded video. 0:34.', 'score': 0.6188962, 'raw_content': None}, {'url': 'https://homes.cs.washington.edu/~lsz/', 'title': 'Luke Zettlemoyer - University of Washington', 'content': 'Jun 6, 2025: I gave a talk on mixed-modal LMs at the Frontiers in NeuroAI workshop at the Kempner Institute. Feb 11, 2025: Congratulations to Chunting and Lili and the Tranfusion team for getting an oral at ICLR 2025! Dec 3, 2024: Elected ACL President (2025 VP Elect; 2026 VP; 2027 Pres; 2028 Past Pres) Aug 11, 2024: Congrats to OLMo team for winning a Best Theme Paper Award at ACL 2024! Aug 11, 2024: Congrats to Dolma team for winning a Best Resource Paper Award at ACL 2024! Jan 15, 2024: [...] Congratulations to Xian and team for getting oral at ICLR 2024 for their work on instruction backtranslation! Oct 10, 2023: I was on the twiml podcast, Episode 650 Sep 21, 2023: Congrauations to Tim and Arti for the QLoRA paper getting an oral at NeurIPS 2023! Sep 21, 2023: Congrauations to Timo and team on the Toolformer paper getting an oral at NeurIPS 2023! Jul 24, 2023: I was on The Thesis Review with Sean Welleck Episode 45 Jan 6, 2022: I was named an ACL Fellow! [...] ACL Fellow (2021), as well as winning a PECASE award (2016), an Allen Distinguished Investigator award (2014), and many best paper awards. I was an undergraduate at NC State, recieved my PhD from MIT, and was a postdocal researcher at the University of Edinburgh.', 'score': 0.6161283, 'raw_content': None}, {'url': 'https://x.com/LukeZettlemoyer/status/1003662931479941120?lang=ar-x-fm', 'title': 'Luke Zettlemoyer - X', 'content': 'Luke Zettlemoyer ¬∑ @LukeZettlemoyer. Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT', 'score': 0.5533369, 'raw_content': None}, {'url': 'https://twitter.com/LukeZettlemoyer/with_replies', 'title': 'Posts with replies by Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': \"Joined September 2015. 2,120 Following ¬∑ 9,556 Followers ¬∑ Posts ¬∑ Replies ¬∑ Media. @LukeZettlemoyer hasn't posted. When they do, their posts will show up\", 'score': 0.37855154, 'raw_content': None}, {'url': 'https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=en', 'title': '\\u202aLuke Zettlemoyer\\u202c - \\u202aGoogle Scholar\\u202c', 'content': \"New articles by this author. New citations to this author. New articles related to this author's research. Email address for updates ... Luke Zettlemoyer.\", 'score': 0.32309222, 'raw_content': None}], 'response_time': 3.61})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Here are the latest tweets or relevant updates for each of the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - I couldn\\'t find a specific tweet from Tim Dettmers, but you can explore his work and updates on his [GitHub profile](https://github.com/timdettmers) or his [personal website](https://timdettmers.com/).\\n\\n2. **Artidoro Pagnoni**:\\n   - Artidoro Pagnoni recently introduced the Byte Latent Transformer (BLT), an LLM architecture that scales better than Llama 3 using byte-patches. You can follow his updates on [Twitter](https://twitter.com/ArtidoroPagnoni).\\n\\n3. **Ari Holtzman**:\\n   - Ari Holtzman tweeted: \"join me\" with an image on December 3, 2024. You can view his profile and updates on [Twitter](https://twitter.com/universeinanegg/status/1864053976729002080).\\n\\n4. **Luke Zettlemoyer**:\\n   - Luke Zettlemoyer introduced FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration. You can follow his updates on [Twitter](https://x.com/lukezettlemoyer?lang=en).\\n\\nFeel free to explore their profiles for more detailed updates and information.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 5140, 'total_tokens': 5430, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt3j2MjRcQy1oep556xVNaaknsECx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--df1d637a-74e9-40c6-8c83-17a4e6466b54-0', usage_metadata={'input_tokens': 5140, 'output_tokens': 290, 'total_tokens': 5430, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "##### ‚úÖ Answer:\n",
        "\n",
        "1. Request sent to agent node and agent node decided it needed to use the arxiv tool with the query Qlora.\n",
        "2. Action node runs arxiv tool with Qlora query and returns results to agent.\n",
        "3. Agent node received results and determines it needs to now call the tavily tool search 4 times, one for each author to find their latest tweets.\n",
        "4. Action node runs the tavily search for the four authors latest tweets.\n",
        "5. Agent node received results decides no more tool calls necessary and moves to END."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1LFJ4xXBqWgLBfpOIq8wak04', 'function': {'arguments': '{\"query\": \"Tesla CEO 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_I1124xYXKkAJgA3FSxxPmzxS', 'function': {'arguments': '{\"query\": \"Elon Musk\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 235, 'total_tokens': 288, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt45gpfCHjX9gk4B86YtdzJWF2ss1', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--a072545f-57ab-4f96-a5f0-8c3ea82e5395-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Tesla CEO 2023'}, 'id': 'call_1LFJ4xXBqWgLBfpOIq8wak04', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Elon Musk'}, 'id': 'call_I1124xYXKkAJgA3FSxxPmzxS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 235, 'output_tokens': 53, 'total_tokens': 288, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: tavily_search_results_json\n",
            "[ToolMessage(content='[{\"title\": \"Who Is Driving Tesla\\'s Leadership Team? - Investopedia\", \"url\": \"https://www.investopedia.com/articles/company-insights/090316/who-driving-teslas-management-team-tsla.asp\", \"content\": \"Vaibhav Taneja is in charge of Tesla‚Äôs finances. He was appointedchief financial officer (CFO) of Tesla in August 2023, taking over from Zachary J. Kirkhorn, a popular figure who was once viewed as a possible successor to Elon Musk as CEO.1 2 [...] There have been a lot of changes at Tesla over the years, including at the top. The only long-serving executive leader at the company is Musk. The other leading positions were filled in 2023. Musk and his old colleagues succeeded in turning Tesla into a serious player in the electric vehicle space and helping to revolutionize the auto industry. Now, alongside his latest right-hand men, Taneja and Zhu, he must fight off competition while remaining profitable and successfully pivoting into new [...] The other leadership executive listed on Tesla‚Äôs website is Tom Zhu. Zhu‚Äôs current job title is senior vice president of automotive, which is a position he‚Äôs held since April 2023 and requires him to oversee global production, sales, deliveries, service, and the company‚Äôs factories. Prior to that, he was based in China, where he was in charge of Tesla‚Äôs Asia Pacific operations.4\", \"score\": 0.84707206}, {\"title\": \"Tesla IncExecutive & Employee Information - GlobalData\", \"url\": \"https://www.globaldata.com/company-profile/tesla-inc/executives/\", \"content\": \"Mr. Vaibhav Taneja has been the Chief Financial Officer of Tesla since August 2023. He also serves as the Chief Accounting Officer of the company since March 2019. Prior to this, he served as Corporate Controller of the company from May 2018 to March 2019. Mr. Taneja also served as Assistant Corporate Controller at Tesla from February 2017 to May 2018 and also served in several finance and accounting roles at SolarCity Corp from March 2016 to February 2017. [...] The following section provides information on Tesla Inc‚Äôs senior management, executives, CEO and key decision makers and their roles in the organization. For more insight into Tesla Inc\\'s management and employees, unlock the full data with our company analytics monitoring tool.\\\\n\\\\nExecutives\\\\n----------\\\\n\\\\n### Elon Musk [...] Mr. Elon Musk has been the Chief Executive Officer since 2008 and Director since 2004 of the company. Prior to this, he co-founded PayPal, and Zip2 Corporation, and was Chairman of the Board of SolarCity Corporation. Currently, he also serves as the Chief Executive Officer, Chief Technology Officer, and Chairman of Space Exploration Technologies Corporation since 2002; Chief Executive Officer of Twitter Inc since 2022; and Director of Endeavor Group Holdings Inc since April 2021.\", \"score\": 0.81770587}, {\"title\": \"Tesla In 2023: Building A Radically Innovative Operating System\", \"url\": \"https://www.gsb.stanford.edu/faculty-research/case-studies/tesla-2023-building-radically-innovative-operating-system\", \"content\": \"For CEO Elon Musk, Tesla‚Äôs mission required not only new technologies to create electric vehicles, but innovation on the software that connected every aspect of the organization. Tesla was founded in 2003 with the goal of revolutionizing the automotive industry, by producing electric vehicles that would help accelerate the world‚Äôs transition to sustainable energy. Twenty years later, Tesla had achieved remarkable progress across multiple dimensions such as production capacity, innovative [...] ## Related\\\\n\\\\nRobert A. Burgelman\\\\n\\\\n### Robert Burgelman\\\\n\\\\nRaj Joshi\\\\n\\\\n### Raj Joshi\\\\n\\\\nHome\\\\n\\\\n## Footer contact links\\\\n\\\\n## Follow Us\\\\n\\\\n## Footer 1\\\\n\\\\n## Footer 2\\\\n\\\\n## Footer legal links\", \"score\": 0.76321954}, {\"title\": \"Elon Musk - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Elon_Musk\", \"content\": \"their leadership in the AI boom in the 2020s led him to establish xAI \\\\\"XAI (company)\\\\\"). In 2022, he acquired the social network Twitter, implementing significant changes and rebranding it as X in 2023. His other businesses include the neurotechnology company Neuralink, which he co-founded in 2016, and the tunneling company the Boring Company, which he founded in 2017. [...] unanimous opposition from congressional Republicans and several Democrats.( In 2022, Musk said he would start supporting Republican Party \\\\\"Republican Party (United States)\\\\\") candidates,( and gave over $50 million to Citizens for Sanity, a conservative political action committee.( In 2023, he supported Republican Ron DeSantis for the 2024 U.S. presidential election, giving $10 million to his campaign,( and hosted DeSantis\\'s campaign announcement on a Twitter Spaces event.( From June 2023 to [...] During the Russian invasion of Ukraine, Musk provided free Starlink service to Ukraine, permitting Internet access and communication at a yearly cost to SpaceX of $400 million.( However, Musk refused to block Russian state media on Starlink.( In 2023, Musk denied Ukraine\\'s request to activate Starlink over Crimea to aid an attack against the Russian navy, citing fears of a nuclear response.(\\\\n\\\\n### Tesla\\\\n\\\\nMain article: Tesla, Inc.\", \"score\": 0.61668247}, {\"title\": \"Elon Musk - Tesla Investor Relations\", \"url\": \"https://ir.tesla.com/corporate/elon-musk\", \"content\": \"acquired by Compaq in March 1999. Elon holds a B.A. in physics from the University of Pennsylvania and a B.S. in business from the Wharton School of the University of Pennsylvania. [...] Elon is Technoking of Tesla and has served as our Chief Executive Officer since October 2008 and as a member of the Board since April 2004. Elon has also served as Chief Executive Officer, Chief Technology Officer and Chairman of Space Exploration Technologies Corporation, an advanced rocket and spacecraft manufacturing and services company (‚ÄúSpaceX ‚Äù), since May 2002, and served as Chairman of the Board of SolarCity Corporation, a solar installation company, from July 2006 until its\", \"score\": 0.5139862}]', name='tavily_search_results_json', id='6b72558a-30df-4913-9c85-39134f2e7bc7', tool_call_id='call_1LFJ4xXBqWgLBfpOIq8wak04', artifact={'query': 'Tesla CEO 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.investopedia.com/articles/company-insights/090316/who-driving-teslas-management-team-tsla.asp', 'title': \"Who Is Driving Tesla's Leadership Team? - Investopedia\", 'content': 'Vaibhav Taneja is in charge of Tesla‚Äôs finances. He was appointedchief financial officer (CFO) of Tesla in August 2023, taking over from Zachary J. Kirkhorn, a popular figure who was once viewed as a possible successor to Elon Musk as CEO.1 2 [...] There have been a lot of changes at Tesla over the years, including at the top. The only long-serving executive leader at the company is Musk. The other leading positions were filled in 2023. Musk and his old colleagues succeeded in turning Tesla into a serious player in the electric vehicle space and helping to revolutionize the auto industry. Now, alongside his latest right-hand men, Taneja and Zhu, he must fight off competition while remaining profitable and successfully pivoting into new [...] The other leadership executive listed on Tesla‚Äôs website is Tom Zhu. Zhu‚Äôs current job title is senior vice president of automotive, which is a position he‚Äôs held since April 2023 and requires him to oversee global production, sales, deliveries, service, and the company‚Äôs factories. Prior to that, he was based in China, where he was in charge of Tesla‚Äôs Asia Pacific operations.4', 'score': 0.84707206, 'raw_content': None}, {'url': 'https://www.globaldata.com/company-profile/tesla-inc/executives/', 'title': 'Tesla IncExecutive & Employee Information - GlobalData', 'content': \"Mr. Vaibhav Taneja has been the Chief Financial Officer of Tesla since August 2023. He also serves as the Chief Accounting Officer of the company since March 2019. Prior to this, he served as Corporate Controller of the company from May 2018 to March 2019. Mr. Taneja also served as Assistant Corporate Controller at Tesla from February 2017 to May 2018 and also served in several finance and accounting roles at SolarCity Corp from March 2016 to February 2017. [...] The following section provides information on Tesla Inc‚Äôs senior management, executives, CEO and key decision makers and their roles in the organization. For more insight into Tesla Inc's management and employees, unlock the full data with our company analytics monitoring tool.\\n\\nExecutives\\n----------\\n\\n### Elon Musk [...] Mr. Elon Musk has been the Chief Executive Officer since 2008 and Director since 2004 of the company. Prior to this, he co-founded PayPal, and Zip2 Corporation, and was Chairman of the Board of SolarCity Corporation. Currently, he also serves as the Chief Executive Officer, Chief Technology Officer, and Chairman of Space Exploration Technologies Corporation since 2002; Chief Executive Officer of Twitter Inc since 2022; and Director of Endeavor Group Holdings Inc since April 2021.\", 'score': 0.81770587, 'raw_content': None}, {'url': 'https://www.gsb.stanford.edu/faculty-research/case-studies/tesla-2023-building-radically-innovative-operating-system', 'title': 'Tesla In 2023: Building A Radically Innovative Operating System', 'content': 'For CEO Elon Musk, Tesla‚Äôs mission required not only new technologies to create electric vehicles, but innovation on the software that connected every aspect of the organization. Tesla was founded in 2003 with the goal of revolutionizing the automotive industry, by producing electric vehicles that would help accelerate the world‚Äôs transition to sustainable energy. Twenty years later, Tesla had achieved remarkable progress across multiple dimensions such as production capacity, innovative [...] ## Related\\n\\nRobert A. Burgelman\\n\\n### Robert Burgelman\\n\\nRaj Joshi\\n\\n### Raj Joshi\\n\\nHome\\n\\n## Footer contact links\\n\\n## Follow Us\\n\\n## Footer 1\\n\\n## Footer 2\\n\\n## Footer legal links', 'score': 0.76321954, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Elon_Musk', 'title': 'Elon Musk - Wikipedia', 'content': 'their leadership in the AI boom in the 2020s led him to establish xAI \"XAI (company)\"). In 2022, he acquired the social network Twitter, implementing significant changes and rebranding it as X in 2023. His other businesses include the neurotechnology company Neuralink, which he co-founded in 2016, and the tunneling company the Boring Company, which he founded in 2017. [...] unanimous opposition from congressional Republicans and several Democrats.( In 2022, Musk said he would start supporting Republican Party \"Republican Party (United States)\") candidates,( and gave over $50 million to Citizens for Sanity, a conservative political action committee.( In 2023, he supported Republican Ron DeSantis for the 2024 U.S. presidential election, giving $10 million to his campaign,( and hosted DeSantis\\'s campaign announcement on a Twitter Spaces event.( From June 2023 to [...] During the Russian invasion of Ukraine, Musk provided free Starlink service to Ukraine, permitting Internet access and communication at a yearly cost to SpaceX of $400 million.( However, Musk refused to block Russian state media on Starlink.( In 2023, Musk denied Ukraine\\'s request to activate Starlink over Crimea to aid an attack against the Russian navy, citing fears of a nuclear response.(\\n\\n### Tesla\\n\\nMain article: Tesla, Inc.', 'score': 0.61668247, 'raw_content': None}, {'url': 'https://ir.tesla.com/corporate/elon-musk', 'title': 'Elon Musk - Tesla Investor Relations', 'content': 'acquired by Compaq in March 1999. Elon holds a B.A. in physics from the University of Pennsylvania and a B.S. in business from the Wharton School of the University of Pennsylvania. [...] Elon is Technoking of Tesla and has served as our Chief Executive Officer since October 2008 and as a member of the Board since April 2004. Elon has also served as Chief Executive Officer, Chief Technology Officer and Chairman of Space Exploration Technologies Corporation, an advanced rocket and spacecraft manufacturing and services company (‚ÄúSpaceX ‚Äù), since May 2002, and served as Chairman of the Board of SolarCity Corporation, a solar installation company, from July 2006 until its', 'score': 0.5139862, 'raw_content': None}], 'response_time': 2.46}), ToolMessage(content='Page: Elon Musk\\nSummary: Elon Reeve Musk  ( EE-lon; born June 28, 1971) is a businessman. He is known for his leadership of Tesla, SpaceX, X (formerly Twitter), and the Department of Government Efficiency (DOGE). Musk has been considered the wealthiest person in the world since 2021; as of May 2025, Forbes estimates his net worth to be US$424.7 billion. \\nBorn to a wealthy family in Pretoria, South Africa, Musk emigrated in 1989 to Canada. He received bachelor\\'s degrees from the University of Pennsylvania in 1997 before moving to California, United States, to pursue business ventures. In 1995, Musk co-founded the software company Zip2. Following its sale in 1999, he co-founded X.com, an online payment company that later merged to form PayPal, which was acquired by eBay in 2002. That year, Musk also became an American citizen.\\nIn 2002, Musk founded the space technology company SpaceX, becoming its CEO and chief engineer; the company has since led innovations in reusable rockets and commercial spaceflight. Musk joined the automaker Tesla as an early investor in 2004 and became its CEO and product architect in 2008; it has since become a leader in electric vehicles. In 2015, he co-founded OpenAI to advance artificial intelligence (AI) research but later left; growing discontent with the organization\\'s direction and their leadership in the AI boom in the 2020s led him to establish xAI. In 2022, he acquired the social network Twitter, implementing significant changes and rebranding it as X in 2023. His other businesses include the neurotechnology company Neuralink, which he co-founded in 2016, and the tunneling company the Boring Company, which he founded in 2017.\\nMusk was the largest donor in the 2024 U.S. presidential election, and is a supporter of global far-right figures, causes, and political parties. In early 2025, he served as senior advisor to United States president Donald Trump and as the de facto head of DOGE. After a public feud with Trump, Musk left the Trump administration and announced he was creating his own political party, the America Party.\\nMusk\\'s political activities, views, and statements have made him a polarizing figure, especially following the COVID-19 pandemic. He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation and promoting conspiracy theories, and affirming antisemitic, racist, and transphobic comments. His acquisition of Twitter was controversial due to a subsequent increase in hate speech and the spread of misinformation on the service. His role in the second Trump administration attracted public backlash, particularly in response to DOGE.\\n\\nPage: Wealth of Elon Musk\\nSummary: Elon Musk is the wealthiest person in the world, with an estimated net worth of US$381 billion as of May 2025, according to the Bloomberg Billionaires Index, and $424.7 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX. \\nHaving been first listed on the Forbes Billionaires List in 2012, around 75% of Musk\\'s wealth was derived from Tesla stock in November 2020. Describing himself as \"cash poor\", he became the first person in the world to have a net worth above $300 billion a year later. By December 2024, he became the first person to reach a net worth of $400 billion.\\n\\nPage: Views of Elon Musk\\nSummary: Elon Musk is the owner of multiple companies, the wealthiest individual in the world, and a former US government employee. Having rejected the conservative label, Musk has described himself as a political moderate; his views have become more right-wing over time, and have been characterized as libertarian and far-right. As the owner of Twitter, he has suppressed critics, and after his involvement in European politics, his views have received criticism from some world leaders. Musk has also expressed his opinion on topics from science and technology to religion and philosophy.\\nWithin the context of American politics, Musk voted for Democratic can', name='wikipedia', id='cebfe505-088a-44b9-a030-fdeaf8c79089', tool_call_id='call_I1124xYXKkAJgA3FSxxPmzxS')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Elon Musk is the CEO of Tesla. According to Wikipedia, Elon Musk was born on June 28, 1971.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2546, 'total_tokens': 2573, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bt45kPGJxaOkfTlkNvHc9NyftSeLt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--45e21578-e91a-46f9-af8d-df538d2e4e5d-0', usage_metadata={'input_tokens': 2546, 'output_tokens': 27, 'total_tokens': 2573, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# testing wikipedia tool\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Use tavily to search for the name of Tesla's CEO, then search wikipedia to find out and return their birthdate!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Retrieval-augmented generation (RAG) is a technique that enhances large language models (LLMs) by allowing them to retrieve and incorporate new information from specified documents before generating responses. This approach enables LLMs to use domain-specific or updated information that is not available in their pre-existing training data. RAG helps improve the accuracy of LLMs by blending the language model process with a document look-up or web search process, thereby reducing AI hallucinations and the need for retraining with new data. It also allows LLMs to include sources in their responses, providing greater transparency and enabling users to verify the cited information. The term RAG was introduced in a 2020 research paper from Meta.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain_with_formatting.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions.\n",
        "##### ‚úÖ Answer:\n",
        "\n",
        "questions = [<br>\n",
        "    \"Who did Elon Musk found Zip2 with?\",<br>\n",
        "    \"Where was Elon Musk born?\",<br>\n",
        "    \"Who are Elon Musk‚Äôs Parents?\",<br>\n",
        "    \"Who did Elon Musk endorse for the 2024 presidential race?\",<br>\n",
        "    \"What government organization was Elon Musk de facto head of?\"<br>\n",
        "]<br>\n",
        "\n",
        "answers = [<br>\n",
        "    {\"must_mention\" : [\"Kimbal\", \"Greg\"]},<br>\n",
        "    {\"must_mention\" : [\"Africa\", \"Pretoria\"]},<br>\n",
        "    {\"must_mention\" : [\"Errol\", \"Maye\"]},<br>\n",
        "    {\"must_mention\" : [\"Republican\", \"Trump\"]},<br>\n",
        "    {\"must_mention\" : [\"Government\", \"Efficiency\"]}<br>\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['e8886f66-8640-4caf-9892-0f059191f604',\n",
              "  '2d265c8f-7d08-468d-b35a-750fa43f3660',\n",
              "  'ed002aa1-6d84-4568-95e0-a5efb3e0c0f0',\n",
              "  '4cf9b5fc-b7c0-44b9-8d6c-baccd4a599b3',\n",
              "  '7d71c2f4-ada8-4e93-b0fe-e935da78d6cc',\n",
              "  'bc1b3968-9a2c-4b64-91f4-e5a044bfd0a0'],\n",
              " 'count': 6}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "##### ‚úÖ Answer:\n",
        "The format is associated by index in both lists. The content is associated by words that we expect, and I suppose will enforce, that our model MUST include in the answers to the provided questions. This is potentially promblematic if the test set is ambiguous or designed \"poorly\" but can also be useful in efforts to enforce certain responses from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required) #returns true if all phrases in the required list are in the prediction\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "##### ‚úÖ Answer:\n",
        "One way to improve the performance measuring capability of the metric as is, is to make the matching non-case sensitive. Another way to improve the metric would be to implement some kind of fuzzy matching, instead of the exact, to be able to capture semantic similarities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Search Pipeline - Evaluation - a9cf-60b7714b' at:\n",
            "https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/compare?selectedSessions=9d8ecb24-0da0-4542-9108-ea418b4b5769\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a57fa8371fa845498366d3bdc42c7d6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"Search Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ExperimentResults Search Pipeline - Evaluation - a9cf-60b7714b>\n"
          ]
        }
      ],
      "source": [
        "experiment_results #not sure what this is supposed to do, so added result output below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'run': RunTree(id=8582c9db-e698-458e-8c8f-7b1e6ac92973, name='Target', run_type='chain', dotted_order='20250714T121603006745Z8582c9db-e698-458e-8c8f-7b1e6ac92973'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=2d265c8f-7d08-468d-b35a-750fa43f3660, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/2d265c8f-7d08-468d-b35a-750fa43f3660'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c1e793ea-a25e-484b-9269-52d6dbe8f8ee'), target_run_id=None, extra=None)]}},\n",
              " {'run': RunTree(id=4261f78a-2e6e-4180-9f01-97241ae24b7f, name='Target', run_type='chain', dotted_order='20250714T121612181789Z4261f78a-2e6e-4180-9f01-97241ae24b7f'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=4cf9b5fc-b7c0-44b9-8d6c-baccd4a599b3, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/4cf9b5fc-b7c0-44b9-8d6c-baccd4a599b3'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('e2f8a9fa-21f3-40f4-ab04-78eb4f0c9266'), target_run_id=None, extra=None)]}},\n",
              " {'run': RunTree(id=24faa13b-37df-4e7a-91c5-3dc859c706be, name='Target', run_type='chain', dotted_order='20250714T121622328243Z24faa13b-37df-4e7a-91c5-3dc859c706be'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=7d71c2f4-ada8-4e93-b0fe-e935da78d6cc, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/7d71c2f4-ada8-4e93-b0fe-e935da78d6cc'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('87fb5a0c-6642-42f2-a37e-ef75e259eca7'), target_run_id=None, extra=None)]}},\n",
              " {'run': RunTree(id=9d12a60f-56f2-4e64-a919-005f904c1877, name='Target', run_type='chain', dotted_order='20250714T121630303118Z9d12a60f-56f2-4e64-a919-005f904c1877'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=bc1b3968-9a2c-4b64-91f4-e5a044bfd0a0, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/bc1b3968-9a2c-4b64-91f4-e5a044bfd0a0'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c085f79c-01f1-4d41-81bb-8d281861844b'), target_run_id=None, extra=None)]}},\n",
              " {'run': RunTree(id=8de4e62c-ee7e-4e95-b047-c360539a6538, name='Target', run_type='chain', dotted_order='20250714T121638600877Z8de4e62c-ee7e-4e95-b047-c360539a6538'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=e8886f66-8640-4caf-9892-0f059191f604, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/e8886f66-8640-4caf-9892-0f059191f604'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2ac1b05a-07ae-4365-a972-b4b7c45a9677'), target_run_id=None, extra=None)]}},\n",
              " {'run': RunTree(id=70ef7e5c-193b-4e0c-9e67-a8baf39631fd, name='Target', run_type='chain', dotted_order='20250714T121653861471Z70ef7e5c-193b-4e0c-9e67-a8baf39631fd'),\n",
              "  'example': <class 'langsmith.schemas.Example'>(id=ed002aa1-6d84-4568-95e0-a5efb3e0c0f0, dataset_id=c1bef398-6e45-4381-9b69-a24da44e0362, link='https://smith.langchain.com/o/b538314a-4903-4172-841d-f92d30cdda3d/datasets/c1bef398-6e45-4381-9b69-a24da44e0362/e/ed002aa1-6d84-4568-95e0-a5efb3e0c0f0'),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ee01849f-4d67-49e9-a021-955033eb5385'), target_run_id=None, extra=None)]}}]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results._results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we start to configure our graph by first initializing the graph and then adding an agent node and an action node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b334d0>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Next we provide an entry point to our graph, specifically the agent node. This will tell our graph where to direct the initial user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b334d0>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we define our conditional edge. (explanation of functionality in activity 4 below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "##### ‚úÖ Answer:\n",
        "This conditional edge first checks the last message in our state similar to the should continue edge. If the last message contains a tool calls kwarg, it returns action. It then sets variables for the first and final response (not sure why we have another variable for the final response when we already have the last message variable). It then checks the length of the messages in the state object and returns END if the lenght of the state in >10 imposing a processing limit on state changes or loops. Next a prompt template is composed where a quesry is given along with the first and last messages to determine with respect to the first message how helpful the last message is. That prompt is given to an open AI model via LCEL and the response is checked for a Y indicating that the model thought it was helpful. If the Y present the edge returns end, and if not it returns continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we add our conditional edge to the graph, from the agent node to the conditional edge. We also provide mappings of the responses from the conditional edge. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b334d0>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we complete our reasoning action loop by inserting an edge into the graph connecting the action node to the agent node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x110b334d0>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we compile our graph. Redefining it as an \"agent\" now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "Here we pass a sample prompt to our graph and output the updates from each node as the graph runs and we traverse through the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_FSIzT9bxaKSrB3Uh1D0zLJjW', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_L9DOxj2DOuNKlFRdcTtdVbtm', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_lMmjVQqdPJp6dlUwmC2XpC32', 'function': {'arguments': '{\"query\": \"Attention (machine learning)\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 235, 'total_tokens': 300, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BtEQ1BWtGK6VVh57fsViUCl0k0JgK', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--035a19ec-cb78-4de7-b95e-9ababc299849-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_FSIzT9bxaKSrB3Uh1D0zLJjW', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_L9DOxj2DOuNKlFRdcTtdVbtm', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Attention (machine learning)'}, 'id': 'call_lMmjVQqdPJp6dlUwmC2XpC32', 'type': 'tool_call'}], usage_metadata={'input_tokens': 235, 'output_tokens': 65, 'total_tokens': 300, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Work/Desktop/ai_bootcamp/code/AIE7/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /Users/Work/Desktop/ai_bootcamp/code/AIE7/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Page: Fine-tuning (deep learning)\\nSummary: In deep learning, fine-tuning is an approach to transfer learning in which the parameters of a pre-trained neural network model are trained on new data. Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are \"frozen\" (i.e., not changed during backpropagation). A model may also be augmented with \"adapters\" that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the model\\'s weights frozen.\\nFor some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.\\nModels that are pre-trained on large, general corpora are usually fine-tuned by reusing their parameters as a starting point and adding a task-specific layer trained from scratch. Fine-tuning the full model is also common and often yields better results, but is more computationally expensive.\\nFine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models such as ChatGPT (a fine-tuned version of GPT models) and Sparrow.\\n\\n\\n\\nPage: List of datasets for machine-learning research\\nSummary: These datasets are used in machine learning (ML) research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.\\nMany organizations, including governments, publish and share their datasets. The datasets are classified, based on the licenses, as Open data and Non-Open data.\\nThe datasets from various governmental-bodies are presented in List of open government data sites. The datasets are ported on open data portals. They are made available for searching, depositing and accessing through interfaces like Open API.  The datasets are made available as various sorted types and subtypes.', name='wikipedia', id='46867812-32d5-40c3-9177-6a6ee01fa50e', tool_call_id='call_FSIzT9bxaKSrB3Uh1D0zLJjW'), ToolMessage(content='Page: Data parallelism\\nSummary: Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel. It can be applied on regular data structures like arrays and matrices by working on each element in parallel. It contrasts to task parallelism as another form of parallelism.\\nA data parallel job on an array of n elements can be divided equally among all the processors. Let us assume we want to sum all the elements of the given array and the time for a single addition operation is Ta time units. In the case of sequential execution, the time taken by the process will be n√óTa time units as it sums up all the elements of an array. On the other hand, if we execute this job as a data parallel job on 4 processors the time taken would reduce to (n/4)√óTa + merging overhead time units. Parallel execution results in a speedup of 4 over sequential execution. The locality of data references plays an important part in evaluating the performance of a data parallel programming model. Locality of data depends on the memory accesses performed by the program as well as the size of the cache.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', name='wikipedia', id='68560633-7fcd-4f2b-984f-f4b6e7ff7fd8', tool_call_id='call_L9DOxj2DOuNKlFRdcTtdVbtm'), ToolMessage(content='Page: Attention (machine learning)\\nSummary: In machine learning, attention is a  method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\n\\n\\nPage: Transformer (deep learning architecture)\\nSummary: In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\\n\\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\\n\\n\\n\\nPage: Machine learning\\nSummary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\\n\\n', name='wikipedia', id='b43cbaa5-02a9-4b89-9e9a-a371fbc1fad5', tool_call_id='call_lMmjVQqdPJp6dlUwmC2XpC32')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_16ZkEsg916Mu4F5b8918Q1hi', 'function': {'arguments': '{\"query\":\"Tim Dettmers\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 2132, 'total_tokens': 2148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BtEQ6lANmEnidTW65dCT9VeaFgM1Q', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2aaf1488-64c7-4564-9db6-0721ab15f3b4-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_16ZkEsg916Mu4F5b8918Q1hi', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2132, 'output_tokens': 16, 'total_tokens': 2148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Page: Data parallelism\\nSummary: Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel. It can be applied on regular data structures like arrays and matrices by working on each element in parallel. It contrasts to task parallelism as another form of parallelism.\\nA data parallel job on an array of n elements can be divided equally among all the processors. Let us assume we want to sum all the elements of the given array and the time for a single addition operation is Ta time units. In the case of sequential execution, the time taken by the process will be n√óTa time units as it sums up all the elements of an array. On the other hand, if we execute this job as a data parallel job on 4 processors the time taken would reduce to (n/4)√óTa + merging overhead time units. Parallel execution results in a speedup of 4 over sequential execution. The locality of data references plays an important part in evaluating the performance of a data parallel programming model. Locality of data depends on the memory accesses performed by the program as well as the size of the cache.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', name='wikipedia', id='62667a84-f7fd-41ce-b1b7-bdfa4f0a20e3', tool_call_id='call_16ZkEsg916Mu4F5b8918Q1hi')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a technique used in the fine-tuning of deep learning models. It involves augmenting a pre-trained neural network with \"adapters\" that consist of far fewer parameters than the original model. This allows for parameter-efficient fine-tuning by adjusting the weights of these adapters while keeping the rest of the model\\'s weights frozen. This approach is particularly useful for adapting large models to new tasks without the need for extensive computational resources.\\n\\n### Attention in Machine Learning\\nAttention is a mechanism in machine learning that determines the importance of each component in a sequence relative to others. In natural language processing, this is represented by \"soft\" weights assigned to each word in a sentence. Attention allows models to focus on relevant parts of the input sequence, improving the handling of dependencies and relationships within the data. It was initially implemented in recurrent neural networks (RNNs) but has become a core component of transformer architectures, which rely on parallel attention schemes for faster processing.\\n\\n### Tim Dettmers\\nUnfortunately, there is no specific information available about Tim Dettmers from the sources accessed. However, Tim Dettmers is known in the machine learning community for his work on efficient deep learning, particularly in the context of model compression and optimization techniques.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 264, 'prompt_tokens': 2679, 'total_tokens': 2943, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2048}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BtEQ9CLPkOuuTaanBTGR5U3GoYQJq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8964566d-b895-47bb-acb3-a458e61945a4-0', usage_metadata={'input_tokens': 2679, 'output_tokens': 264, 'total_tokens': 2943, 'input_token_details': {'audio': 0, 'cache_read': 2048}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Prompt Engineering** is the process of crafting instructions to produce the best possible output from a generative AI model. It involves structuring natural language text to describe the task an AI should perform, whether it's a query, command, or a detailed statement including context and instructions. This technique is crucial for interacting with models like text-to-text, text-to-image, or text-to-audio, where prompts guide the AI to generate desired outputs.\n",
            "\n",
            "**History and Emergence:**\n",
            "Prompt engineering has its roots in the early days of AI, starting with rule-based inputs in the 1950s. It evolved significantly with advancements in machine learning, recurrent neural networks, and deep learning innovations in the 2000s. The development of transformer architectures and GPT models around 2017 marked a significant leap, making prompt engineering a more structured research domain. The release of models like ChatGPT in 2022 further highlighted its importance as a business skill, although its economic future remains uncertain.\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a technique designed to enhance large language models (LLMs) by allowing them to retrieve and incorporate new information before generating responses. This approach helps LLMs access domain-specific and updated information that isn't available in their pre-existing training data. RAG improves the accuracy and reliability of AI-generated responses by integrating retrieval mechanisms with generative capabilities, thus addressing limitations like hallucinations and outdated knowledge.\n",
            "\n",
            "RAG was first introduced in a 2020 research paper by Meta (formerly Facebook AI Research). This method allows LLMs to pull relevant text from databases, uploaded documents, or web sources, reducing the need to retrain models with new data and providing greater transparency by including sources in responses. This approach has rapidly evolved since its inception, incorporating techniques like Dense Passage Retrieval (DPR) and hybrid search methods.\n",
            "\n",
            "For more detailed information, you can refer to the [Wikipedia page on Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) or explore articles like [this one on Medium](https://medium.com/@FrankGoortani/the-evolution-advancements-and-industry-landscape-of-retrieval-augmented-generation-rag-dde4cb39940c).\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Work/Desktop/ai_bootcamp/code/AIE7/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /Users/Work/Desktop/ai_bootcamp/code/AIE7/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning in deep learning is an approach to transfer learning where the parameters of a pre-trained neural network model are trained on new data. This can be done on the entire network or just a subset of its layers, with the rest being \"frozen\" (i.e., not changed during backpropagation). Fine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. It can be combined with reinforcement learning from human feedback to produce models like ChatGPT.\n",
            "\n",
            "Fine-tuning became more prominent with the rise of deep learning and transfer learning techniques. The concept of transfer learning, which fine-tuning is a part of, has been around for a while, but it gained significant attention with the development of large pre-trained models like BERT and GPT in the late 2010s. These models demonstrated the effectiveness of fine-tuning for specific tasks, leading to its widespread adoption in various AI applications.\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents refer to intelligent agents that utilize large language models (LLMs) for natural language processing tasks, particularly language generation. These agents are designed to perceive their environment, take actions autonomously to achieve goals, and may improve their performance through machine learning or by acquiring knowledge. LLMs, such as generative pretrained transformers (GPTs), are commonly used in generative chatbots like ChatGPT, Gemini, or Claude.\n",
            "\n",
            "The concept of LLM-based agents emerged with the development and deployment of large language models, which have been evolving over the past few years. The breakthrough in LLMs can be traced back to the introduction of models like OpenAI's GPT-3 in 2020, which significantly advanced the capabilities of language models in generating human-like text and performing a wide range of language tasks.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
